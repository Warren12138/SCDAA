{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable as V\n",
    "from lib.Exercise1_1 import LQRSolver\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import time \n",
    "\n",
    "Proj_dtype = torch.double\n",
    "Proj_device = 'cpu' \n",
    "class DGMNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DGMNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(3, 100)  \n",
    "        self.layer2 = nn.Linear(100, 100)\n",
    "        self.layer3 = nn.Linear(100, 100)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(100, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.tanh(self.layer2(x))\n",
    "        x = self.relu(self.layer3(x))\n",
    "\n",
    "        return self.output(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGMNN2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DGMNN2, self).__init__()\n",
    "        self.layer1 = nn.Linear(3, 100)\n",
    "        self.layer2 = nn.Linear(100, 100)\n",
    "        self.layer3 = nn.Linear(100, 100)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(100, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        out1 = self.relu(self.layer1(x))\n",
    "        identity = out1\n",
    "        out2 = self.tanh(self.layer2(out1)+identity)\n",
    "        identity = out2\n",
    "        out3 = self.relu(self.layer3(out2)+identity)\n",
    "        return self.output(out3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hessian(grad,x):\n",
    "    Hessian = torch.tensor([], device = Proj_device)\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        hessian = torch.tensor([], device = Proj_device)\n",
    "        for j in range(len(grad[i])):\n",
    "            u_xxi = torch.autograd.grad(grad[i][j], x, grad_outputs=torch.ones_like(grad[i][j]), retain_graph=True,create_graph=True, allow_unused=True)[0]           \n",
    "            hessian = torch.cat((hessian, u_xxi[i].unsqueeze(0)))\n",
    "        Hessian = torch.cat((Hessian, hessian.unsqueeze(0)),dim = 0)\n",
    "        #print(Hessian)\n",
    "    return Hessian\n",
    "\n",
    "def get_hessian_(model,t,x):\n",
    "    Hessian = torch.tensor([], device = Proj_device)\n",
    "    for i in range(len(t)):\n",
    "        x_i = V(x[i],requires_grad=True)\n",
    "        input = torch.cat(((t[i]).unsqueeze(0), x_i),dim=0)\n",
    "        u_in = model(input)\n",
    "        grad = torch.autograd.grad(u_in, x_i, grad_outputs=torch.ones_like(u_in), create_graph=True, retain_graph=True)[0]\n",
    "        hessian = torch.tensor([], device = Proj_device)\n",
    "        for j in range(len(grad)):\n",
    "            u_xxi = torch.autograd.grad(grad[j], x_i, grad_outputs=torch.ones_like(grad[j]), retain_graph=True,create_graph=True, allow_unused=True)[0]           \n",
    "            hessian = torch.cat((hessian, u_xxi.unsqueeze(0)))\n",
    "        Hessian = torch.cat((Hessian, hessian.unsqueeze(0)),dim = 0)\n",
    "    return Hessian\n",
    "\n",
    "def pde_residual(model, t, x):\n",
    "    \n",
    "    input = torch.cat((t.unsqueeze(1), x),dim=1)\n",
    "    \n",
    "    u = model(input)\n",
    "\n",
    "    u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), create_graph=True, retain_graph=True)[0]\n",
    "    u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    u_xx = get_hessian(u_x,x)\n",
    "\n",
    "#    u_xx = get_hessian_(model,t,x)\n",
    " \n",
    "    residual = u_t + 0.5 * torch.einsum('bii->b', sigma @ sigma.transpose(1,2) @ u_xx) + (u_x.unsqueeze(1) @ (H @ x.unsqueeze(1).transpose(1,2)) + u_x.unsqueeze(1) @ M @ alpha + x.unsqueeze(1) @ C @ x.unsqueeze(1).transpose(1,2) + alpha.transpose(1,2) @ D @ alpha).squeeze()\n",
    "    return residual\n",
    "\n",
    "def boundary_condition(model,t, x):\n",
    "\n",
    "    \n",
    "    T_input = T * torch.ones_like(t)\n",
    "\n",
    "    input = torch.cat((T_input.unsqueeze(1), x),dim=1)\n",
    "    u = model(input)\n",
    "\n",
    "    return u - (x.unsqueeze(1) @ R @ x.unsqueeze(1).transpose(1,2)).squeeze()\n",
    "\n",
    "def total_residual(model, t, x):\n",
    "    \n",
    "    residual_loss = pde_residual(model, t, x).pow(2).mean()\n",
    "    boundary_loss = boundary_condition(model,t,x).pow(2).mean()\n",
    "    \n",
    "    return residual_loss + boundary_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_data(num_samples):\n",
    "    #num_samples = 10000\n",
    "    t_samples = T * torch.rand(num_samples, dtype=Proj_dtype, device = Proj_device, requires_grad=True)\n",
    "    x_ends = torch.tensor([-3,3], dtype = Proj_dtype)\n",
    "    x_samples = x_ends[0] + (x_ends[1]- x_ends[0]) * torch.rand(num_samples , 2, dtype=Proj_dtype, device = Proj_device, requires_grad=True)\n",
    "    return t_samples,x_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define matrices for LQR problem\n",
    "\n",
    "H = torch.tensor([[1.2, 0.8], [-0.6, 0.9]], dtype=Proj_dtype, device = Proj_device)\n",
    "M = torch.tensor([[0.5,0.7], [0.3,1.0]], dtype=Proj_dtype, device = Proj_device)\n",
    "sigma = torch.tensor([[[0.08],[0.11]]], dtype=Proj_dtype, device = Proj_device)\n",
    "alpha = torch.tensor([[[1],[1]]], dtype=Proj_dtype, device = Proj_device)\n",
    "C = torch.tensor([[1.6, 0.0], [0.0, 1.1]], dtype=Proj_dtype, device = Proj_device)\n",
    "D = torch.tensor([[0.5, 0.0], [0.0, 0.7]], dtype=Proj_dtype, device = Proj_device)\n",
    "R = torch.tensor([[0.9, 0.0], [0.0, 1.0]], dtype=Proj_dtype, device = Proj_device)\n",
    "T = torch.tensor(1.0, dtype=Proj_dtype, device = Proj_device)\n",
    "\n",
    "solver = LQRSolver(H, M, sigma, C, D, R, T=T, method=\"euler\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/5\n",
      "\n",
      "Epoch 1/40 \t Loss: 145.69715739763402\n",
      "Epoch 5/40 \t Loss: 66.20720361776023\n",
      "Epoch 10/40 \t Loss: 38.23168734814466\n",
      "Epoch 15/40 \t Loss: 31.96301685946888\n",
      "Epoch 20/40 \t Loss: 27.2393708572424\n",
      "Epoch 25/40 \t Loss: 25.740982921180745\n",
      "Epoch 30/40 \t Loss: 24.990316015045266\n",
      "Epoch 35/40 \t Loss: 24.288966269675264\n",
      "Epoch 40/40 \t Loss: 24.185894415126626\n",
      "\n",
      "\n",
      "Batch 2/5\n",
      "\n",
      "Epoch 1/40 \t Loss: 28.795524372230517\n",
      "Epoch 5/40 \t Loss: 28.491295461753374\n",
      "Epoch 10/40 \t Loss: 28.446123398215747\n",
      "Epoch 15/40 \t Loss: 28.114670823715777\n",
      "Epoch 20/40 \t Loss: 27.862256989900654\n",
      "Epoch 25/40 \t Loss: 27.645331215986527\n",
      "Epoch 30/40 \t Loss: 27.656637998965664\n",
      "Epoch 35/40 \t Loss: 27.623298732092955\n",
      "Epoch 40/40 \t Loss: 27.625016687365104\n",
      "\n",
      "\n",
      "Batch 3/5\n",
      "\n",
      "Epoch 1/40 \t Loss: 40.094861584739874\n",
      "Epoch 5/40 \t Loss: 40.07730080631386\n",
      "Epoch 10/40 \t Loss: 40.056054930069024\n",
      "Epoch 15/40 \t Loss: 40.041213736420715\n",
      "Epoch 20/40 \t Loss: 40.264288860483404\n",
      "Epoch 25/40 \t Loss: 40.258832156651245\n",
      "Epoch 30/40 \t Loss: 40.25564200592205\n",
      "Epoch 35/40 \t Loss: 40.25379024488379\n",
      "Epoch 40/40 \t Loss: 40.22109769828735\n",
      "\n",
      "\n",
      "Batch 4/5\n",
      "\n",
      "Epoch 1/40 \t Loss: 37.016958420706445\n",
      "Epoch 5/40 \t Loss: 37.01664679561291\n",
      "Epoch 10/40 \t Loss: 37.01638628679217\n",
      "Epoch 15/40 \t Loss: 37.0162276752898\n",
      "Epoch 20/40 \t Loss: 37.01613329716646\n",
      "Epoch 25/40 \t Loss: 37.016077735688384\n",
      "Epoch 30/40 \t Loss: 37.01604518521354\n",
      "Epoch 35/40 \t Loss: 37.01602615369994\n",
      "Epoch 40/40 \t Loss: 37.01601503244231\n",
      "\n",
      "\n",
      "Batch 5/5\n",
      "\n",
      "Epoch 1/40 \t Loss: 31.687600068772394\n",
      "Epoch 5/40 \t Loss: 31.687598219434747\n",
      "Epoch 10/40 \t Loss: 31.68759659888785\n",
      "Epoch 15/40 \t Loss: 31.68759557469967\n",
      "Epoch 20/40 \t Loss: 31.687594948635876\n",
      "Epoch 25/40 \t Loss: 31.68759457243853\n",
      "Epoch 30/40 \t Loss: 31.6875943484345\n",
      "Epoch 35/40 \t Loss: 31.687594215703093\n",
      "Epoch 40/40 \t Loss: 31.687594137257804\n",
      "\n",
      "\n",
      "Batch 1/5\n",
      "\n",
      "Epoch 1/40 \t Loss: 140.25804603548062\n",
      "Epoch 5/40 \t Loss: 41.707282684146705\n",
      "Epoch 10/40 \t Loss: 31.0237117352176\n",
      "Epoch 15/40 \t Loss: 26.674267131617484\n",
      "Epoch 20/40 \t Loss: 25.006719227841987\n",
      "Epoch 25/40 \t Loss: 24.145867978810077\n",
      "Epoch 30/40 \t Loss: 23.97549528843296\n",
      "Epoch 35/40 \t Loss: 23.643133055022552\n",
      "Epoch 40/40 \t Loss: 23.330774752952838\n",
      "\n",
      "\n",
      "Batch 2/5\n",
      "\n",
      "Epoch 1/40 \t Loss: 23.647720373999523\n",
      "Epoch 5/40 \t Loss: 23.522096543979444\n",
      "Epoch 10/40 \t Loss: 23.379579069623457\n",
      "Epoch 15/40 \t Loss: 23.243759602980756\n",
      "Epoch 20/40 \t Loss: 23.248727645242035\n",
      "Epoch 25/40 \t Loss: 23.20186083118768\n",
      "Epoch 30/40 \t Loss: 23.215786763029108\n",
      "Epoch 35/40 \t Loss: 23.17063231844424\n",
      "Epoch 40/40 \t Loss: 23.24187235846759\n",
      "\n",
      "\n",
      "Batch 3/5\n",
      "\n",
      "Epoch 1/40 \t Loss: 22.319670989968103\n",
      "Epoch 5/40 \t Loss: 22.313929144630578\n",
      "Epoch 10/40 \t Loss: 22.392500038109155\n",
      "Epoch 15/40 \t Loss: 22.334336944213693\n",
      "Epoch 20/40 \t Loss: 22.371885825140666\n",
      "Epoch 25/40 \t Loss: 22.366426029929045\n",
      "Epoch 30/40 \t Loss: 22.360329629534007\n",
      "Epoch 35/40 \t Loss: 22.320717662629683\n",
      "Epoch 40/40 \t Loss: 22.367928212126245\n",
      "\n",
      "\n",
      "Batch 4/5\n",
      "\n",
      "Epoch 1/40 \t Loss: 25.26961614685113\n",
      "Epoch 5/40 \t Loss: 25.223685024041473\n",
      "Epoch 10/40 \t Loss: 25.276448272068578\n",
      "Epoch 15/40 \t Loss: 25.26677877373038\n",
      "Epoch 20/40 \t Loss: 25.25446077364365\n",
      "Epoch 25/40 \t Loss: 25.226988892555823\n",
      "Epoch 30/40 \t Loss: 25.25853830078252\n",
      "Epoch 35/40 \t Loss: 25.313349218874997\n",
      "Epoch 40/40 \t Loss: 25.296765831767487\n",
      "\n",
      "\n",
      "Batch 5/5\n",
      "\n",
      "Epoch 1/40 \t Loss: 23.91018191282457\n",
      "Epoch 5/40 \t Loss: 23.878805494362542\n",
      "Epoch 10/40 \t Loss: 23.940551662943573\n",
      "Epoch 15/40 \t Loss: 23.90429324992658\n",
      "Epoch 20/40 \t Loss: 23.948678340098297\n",
      "Epoch 25/40 \t Loss: 23.87909518644087\n",
      "Epoch 30/40 \t Loss: 23.85243657752183\n",
      "Epoch 35/40 \t Loss: 23.927013552634307\n",
      "Epoch 40/40 \t Loss: 23.882598616701628\n",
      "\n",
      "\n",
      "Batch 1/5\n",
      "\n",
      "Epoch 1/40 \t Loss: 82.28187017901038\n",
      "Epoch 5/40 \t Loss: 17.406256955909967\n",
      "Epoch 10/40 \t Loss: 15.454406731761392\n",
      "Epoch 15/40 \t Loss: 14.627334900130839\n",
      "Epoch 20/40 \t Loss: 14.296309101552259\n",
      "Epoch 25/40 \t Loss: 14.18966453608331\n",
      "Epoch 30/40 \t Loss: 14.139655815433503\n",
      "Epoch 35/40 \t Loss: 14.086685437650925\n",
      "Epoch 40/40 \t Loss: 14.028900222323108\n",
      "\n",
      "\n",
      "Batch 2/5\n",
      "\n",
      "Epoch 1/40 \t Loss: 14.293720333654585\n",
      "Epoch 5/40 \t Loss: 14.246533881774033\n",
      "Epoch 10/40 \t Loss: 14.209934404869397\n",
      "Epoch 15/40 \t Loss: 14.19203377295911\n",
      "Epoch 20/40 \t Loss: 14.194315819660602\n",
      "Epoch 25/40 \t Loss: 14.242540981570233\n",
      "Epoch 30/40 \t Loss: 14.223865828330394\n",
      "Epoch 35/40 \t Loss: 14.238494790388865\n",
      "Epoch 40/40 \t Loss: 14.17343622047862\n",
      "\n",
      "\n",
      "Batch 3/5\n",
      "\n",
      "Epoch 1/40 \t Loss: 13.946444797403506\n",
      "Epoch 5/40 \t Loss: 13.934623963835898\n",
      "Epoch 10/40 \t Loss: 13.940832221867993\n",
      "Epoch 15/40 \t Loss: 13.974564431404712\n",
      "Epoch 20/40 \t Loss: 13.970155282864818\n",
      "Epoch 25/40 \t Loss: 14.010330841972301\n",
      "Epoch 30/40 \t Loss: 13.943030357630326\n",
      "Epoch 35/40 \t Loss: 13.92027814504949\n",
      "Epoch 40/40 \t Loss: 13.914830596828835\n",
      "\n",
      "\n",
      "Batch 4/5\n",
      "\n",
      "Epoch 1/40 \t Loss: 14.172829468552752\n",
      "Epoch 5/40 \t Loss: 14.170890246619635\n",
      "Epoch 10/40 \t Loss: 14.15729535549151\n",
      "Epoch 15/40 \t Loss: 14.205975056601535\n",
      "Epoch 20/40 \t Loss: 14.20302154301812\n",
      "Epoch 25/40 \t Loss: 14.191508211021263\n",
      "Epoch 30/40 \t Loss: 14.192901774689659\n",
      "Epoch 35/40 \t Loss: 14.182002132305822\n",
      "Epoch 40/40 \t Loss: 14.176798337274528\n",
      "\n",
      "\n",
      "Batch 5/5\n",
      "\n",
      "Epoch 1/40 \t Loss: 14.151425007212467\n",
      "Epoch 5/40 \t Loss: 14.122463740978219\n",
      "Epoch 10/40 \t Loss: 14.135920039929474\n",
      "Epoch 15/40 \t Loss: 14.124308438445269\n",
      "Epoch 20/40 \t Loss: 14.1543493088386\n",
      "Epoch 25/40 \t Loss: 14.111061822589232\n",
      "Epoch 30/40 \t Loss: 14.122734494880442\n",
      "Epoch 35/40 \t Loss: 14.133120529026446\n",
      "Epoch 40/40 \t Loss: 14.17837386561494\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samplesize = [100,1000,5000]\n",
    "\n",
    "for sz in samplesize:\n",
    "\n",
    "\n",
    "    model_DGM = DGMNN2().double()\n",
    "    # stat_dict = torch.load('model2_DGM_state_dict.pt', map_location=torch.device('cpu'))\n",
    "    # model_DGM.load_state_dict(stat_dict)\n",
    "    #model_DGM = DGMNN().float().to(Proj_device)\n",
    "    # Prepare for training\n",
    "    optimizer_DGM = torch.optim.Adam(model_DGM.parameters(), lr=0.01)\n",
    "    scheduler_DGM = lr_scheduler.ExponentialLR(optimizer_DGM, gamma=0.9)\n",
    "\n",
    "\n",
    "    epoch_losses = []\n",
    "    # 新建文本文件,f'epochloss{%d}'sz\n",
    "    # 新建文本文件用于存储epoch的损失\n",
    "    loss_filename = f'epoch_loss_{sz}.txt'\n",
    "    with open(loss_filename, 'w') as file:\n",
    "        file.write('Epoch,Loss\\n')\n",
    "\n",
    "    Batch_size = 5\n",
    "    epochs = 40\n",
    "\n",
    "    for batch in range(Batch_size):\n",
    "    \n",
    "        print(f'Batch {batch+1}/{Batch_size}'+'\\n')\n",
    "        \n",
    "        t_data,x_data = new_data(sz)\n",
    "        dataset = TensorDataset(t_data,x_data)\n",
    "        dataloader = DataLoader(dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            model_DGM.train()\n",
    "            total_loss = 0\n",
    "            \n",
    "            for batch_idx, (t_data_,x_data_) in enumerate(dataloader):\n",
    "                optimizer_DGM.zero_grad()\n",
    "                t_v = V(t_data_,requires_grad=True)\n",
    "                x_v = V(x_data_,requires_grad=True)\n",
    "                loss = total_residual(model_DGM, t_v, x_v) \n",
    "                loss.backward(retain_graph=False)\n",
    "                #loss.backward(retain_graph=True)\n",
    "                optimizer_DGM.step()\n",
    "                total_loss += loss.item()\n",
    "            epoch_losses.append(total_loss / len(dataloader))\n",
    "            \n",
    "            scheduler_DGM.step()\n",
    "            if epoch == 0:\n",
    "                print(f'Epoch {epoch+1}/{epochs} \\t Loss: {total_loss / len(dataloader)}')\n",
    "            if(epoch+1)% 10 == 0:\n",
    "                torch.save(model_DGM.state_dict(), f'model2_DGM_state_dict_{sz}.pt')\n",
    "            if (epoch+1) % 5 == 0:\n",
    "                print(f'Epoch {epoch+1}/{epochs} \\t Loss: {total_loss / len(dataloader)}')\n",
    "\n",
    "        print('\\n')\n",
    "        # sdhsl\n",
    "        # 在每个批次结束后，以追加模式将epoch损失数据添加到文件\n",
    "        with open(loss_filename, 'a') as file:\n",
    "            for i, loss in enumerate(epoch_losses, 1):\n",
    "                file.write(f'{i},{loss}\\n')\n",
    "    model_DGM.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Exercise3/value_numerical/3x3/interval_setting.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m load_interval_setting \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mExercise3/value_numerical/3x3/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minterval_setting.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m t_ends \u001b[38;5;241m=\u001b[39m load_interval_setting[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt_ends\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      3\u001b[0m t_num \u001b[38;5;241m=\u001b[39m load_interval_setting[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt_num\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\warre\\Documents\\SCDAA-1\\.venv\\Lib\\site-packages\\torch\\serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\warre\\Documents\\SCDAA-1\\.venv\\Lib\\site-packages\\torch\\serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    447\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\warre\\Documents\\SCDAA-1\\.venv\\Lib\\site-packages\\torch\\serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 426\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Exercise3/value_numerical/3x3/interval_setting.pt'"
     ]
    }
   ],
   "source": [
    "load_interval_setting = torch.load('Exercise3/value_numerical/3x3/'+'interval_setting.pt')\n",
    "t_ends = load_interval_setting['t_ends']\n",
    "t_num = load_interval_setting['t_num']\n",
    "x_ends = load_interval_setting['x_ends']\n",
    "x_num = load_interval_setting['x_num']\n",
    "\n",
    "# Establish meshgrid structure from setting.\n",
    "\n",
    "t_batch_i = torch.linspace(t_ends[0],t_ends[1],t_num,dtype=torch.double)\n",
    "t_batch = t_batch_i.repeat_interleave(x_num[0]*x_num[1])\n",
    "\n",
    "x1 = torch.linspace(x_ends[0][0],x_ends[0][1],x_num[0],dtype=torch.double)\n",
    "x2 = torch.linspace(x_ends[1][0],x_ends[1][1],x_num[1],dtype=torch.double)\n",
    "\n",
    "x_batch = torch.cartesian_prod(x1, x2).unsqueeze(1).repeat(t_num, 1, 1)\n",
    "\n",
    "x_batch_i = torch.cartesian_prod(x1, x2).unsqueeze(1)\n",
    "\n",
    "X1 = x_batch_i[:, 0, 0].view(x_num[0], x_num[1])\n",
    "X2 = x_batch_i[:, 0, 1].view(x_num[0], x_num[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path_MC_FSS = 'Exercise3'+ '/' + f'value_MC/{x_num[0]}x{x_num[1]}/FSS_1e5'\n",
    "\n",
    "FSS_VTSN = [int(x) for x in[5e3]]\n",
    "\n",
    "MSE_FSS_VTSN = []\n",
    "J = []\n",
    "for i in FSS_VTSN:\n",
    "    if i == 1:\n",
    "        trvlthg = ''\n",
    "    else:\n",
    "        trvlthg = 's'\n",
    "    path_FSS_VTSN_i = f\"{file_path_MC_FSS}/{i}_step{trvlthg}\"\n",
    "\n",
    "    J_load_test = torch.load(path_FSS_VTSN_i + '/value_MC.pt')\n",
    "    \n",
    "    J_processed,_ = torch.min(J_load_test,dim = 1)\n",
    "\n",
    "    J.append(J_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1.3017e+04, 2.3263e+03, 5.1811e+02, 4.4414e+03, 3.0043e+00, 4.4438e+03,\n",
       "         5.1807e+02, 2.3284e+03, 1.3023e+04, 6.4233e+03, 1.1970e+03, 2.8020e+02,\n",
       "         2.1580e+03, 1.9030e+00, 2.1573e+03, 2.8078e+02, 1.1973e+03, 6.4229e+03,\n",
       "         3.1621e+03, 6.2128e+02, 1.6471e+02, 1.0424e+03, 1.2883e+00, 1.0420e+03,\n",
       "         1.6474e+02, 6.2202e+02, 3.1622e+03, 1.5508e+03, 3.2614e+02, 1.0586e+02,\n",
       "         5.0345e+02, 9.3517e-01, 5.0310e+02, 1.0581e+02, 3.2611e+02, 1.5509e+03,\n",
       "         7.5768e+02, 1.7330e+02, 7.3826e+01, 2.4318e+02, 7.0335e-01, 2.4313e+02,\n",
       "         7.3837e+01, 1.7329e+02, 7.5778e+02, 3.6838e+02, 9.3428e+01, 5.4422e+01,\n",
       "         1.1848e+02, 5.2839e-01, 1.1849e+02, 5.4438e+01, 9.3441e+01, 3.6838e+02,\n",
       "         1.7743e+02, 5.0974e+01, 4.1271e+01, 5.8761e+01, 3.8132e-01, 5.8735e+01,\n",
       "         4.1279e+01, 5.0971e+01, 1.7738e+02, 8.4289e+01, 2.8008e+01, 3.1423e+01,\n",
       "         3.0081e+01, 2.4854e-01, 3.0077e+01, 3.1427e+01, 2.8018e+01, 8.4262e+01,\n",
       "         3.9005e+01, 1.5326e+01, 2.3572e+01, 1.6088e+01, 1.2278e-01, 1.6085e+01,\n",
       "         2.3573e+01, 1.5324e+01, 3.9017e+01], dtype=torch.float64)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15.4420, 15.4490,  9.3243, 17.8923,  9.3380, 11.3787, 13.3540, 11.6964,\n",
       "         7.2905, 14.0925, 14.6957,  8.7488, 16.3328,  8.7053, 10.9480, 12.1048,\n",
       "        10.8801,  7.1401, 12.8142, 13.8359,  8.1622, 14.8462,  8.0783, 10.5105,\n",
       "        10.9401, 10.1062,  6.9548, 11.5975, 12.8827,  7.5455, 13.3848,  7.4557,\n",
       "        10.0324,  9.8203,  9.3598,  6.7470, 10.4355, 11.7823,  6.9146, 11.9831,\n",
       "         6.8559,  9.5029,  8.7341,  8.6242,  6.4242,  9.2421, 10.6038,  6.2761,\n",
       "        10.6367,  6.4843,  8.9168,  7.6693,  7.8969,  6.0483,  7.9966,  9.3562,\n",
       "         5.5693,  9.3934,  6.1552,  8.2874,  6.5904,  7.1690,  5.6146,  6.7441,\n",
       "         8.1041,  4.8405,  8.1606,  5.8692,  7.5470,  5.5409,  6.4444,  5.1182,\n",
       "         5.4942,  6.7311,  4.0966,  7.0207,  5.6070,  6.6686,  4.4892,  5.7129,\n",
       "         4.5791], dtype=torch.float64, grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_data = t_batch\n",
    "x_data = x_batch.squeeze(1)\n",
    "model_DGM(torch.cat((t_data.unsqueeze(1), x_data),dim=1)).squeeze()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
