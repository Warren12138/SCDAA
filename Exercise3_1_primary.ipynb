{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable as V\n",
    "from lib.Exercise1_1 import LQRSolver\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import time \n",
    "\n",
    "Proj_dtype = torch.double\n",
    "Proj_device = 'cpu' \n",
    "class DGMNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DGMNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(3, 100)  \n",
    "        self.layer2 = nn.Linear(100, 100)\n",
    "        self.layer3 = nn.Linear(100, 100)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(100, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.tanh(self.layer2(x))\n",
    "        x = self.relu(self.layer3(x))\n",
    "\n",
    "        return self.output(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGMNN2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DGMNN2, self).__init__()\n",
    "        self.layer1 = nn.Linear(3, 100)\n",
    "        self.layer2 = nn.Linear(100, 100)\n",
    "        self.layer3 = nn.Linear(100, 100)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(100, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        out1 = self.relu(self.layer1(x))\n",
    "        identity = out1\n",
    "        out2 = self.tanh(self.layer2(out1)+identity)\n",
    "        identity = out2\n",
    "        out3 = self.relu(self.layer3(out2)+identity)\n",
    "        return self.output(out3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hessian(grad,x):\n",
    "    Hessian = torch.tensor([], device = Proj_device)\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        hessian = torch.tensor([], device = Proj_device)\n",
    "        for j in range(len(grad[i])):\n",
    "            u_xxi = torch.autograd.grad(grad[i][j], x, grad_outputs=torch.ones_like(grad[i][j]), retain_graph=True,create_graph=True, allow_unused=True)[0]           \n",
    "            hessian = torch.cat((hessian, u_xxi[i].unsqueeze(0)))\n",
    "        Hessian = torch.cat((Hessian, hessian.unsqueeze(0)),dim = 0)\n",
    "        #print(Hessian)\n",
    "    return Hessian\n",
    "\n",
    "def get_hessian_(model,t,x):\n",
    "    Hessian = torch.tensor([], device = Proj_device)\n",
    "    for i in range(len(t)):\n",
    "        x_i = V(x[i],requires_grad=True)\n",
    "        input = torch.cat(((t[i]).unsqueeze(0), x_i),dim=0)\n",
    "        u_in = model(input)\n",
    "        grad = torch.autograd.grad(u_in, x_i, grad_outputs=torch.ones_like(u_in), create_graph=True, retain_graph=True)[0]\n",
    "        hessian = torch.tensor([], device = Proj_device)\n",
    "        for j in range(len(grad)):\n",
    "            u_xxi = torch.autograd.grad(grad[j], x_i, grad_outputs=torch.ones_like(grad[j]), retain_graph=True,create_graph=True, allow_unused=True)[0]           \n",
    "            hessian = torch.cat((hessian, u_xxi.unsqueeze(0)))\n",
    "        Hessian = torch.cat((Hessian, hessian.unsqueeze(0)),dim = 0)\n",
    "    return Hessian\n",
    "\n",
    "def pde_residual(model, t, x):\n",
    "    \n",
    "    input = torch.cat((t.unsqueeze(1), x),dim=1)\n",
    "    \n",
    "    u = model(input)\n",
    "\n",
    "    u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), create_graph=True, retain_graph=True)[0]\n",
    "    u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    u_xx = get_hessian(u_x,x)\n",
    "\n",
    "#    u_xx = get_hessian_(model,t,x)\n",
    " \n",
    "    residual = u_t + 0.5 * torch.einsum('bii->b', sigma @ sigma.transpose(1,2) @ u_xx) + (u_x.unsqueeze(1) @ (H @ x.unsqueeze(1).transpose(1,2)) + u_x.unsqueeze(1) @ M @ alpha + x.unsqueeze(1) @ C @ x.unsqueeze(1).transpose(1,2) + alpha.transpose(1,2) @ D @ alpha).squeeze()\n",
    "    return residual\n",
    "\n",
    "def boundary_condition(model,t, x):\n",
    "\n",
    "    \n",
    "    T_input = T * torch.ones_like(t)\n",
    "\n",
    "    input = torch.cat((T_input.unsqueeze(1), x),dim=1)\n",
    "    u = model(input)\n",
    "\n",
    "    return u - (x.unsqueeze(1) @ R @ x.unsqueeze(1).transpose(1,2)).squeeze()\n",
    "\n",
    "def total_residual(model, t, x):\n",
    "    \n",
    "    residual_loss = pde_residual(model, t, x).pow(2).mean()\n",
    "    boundary_loss = boundary_condition(model,t,x).pow(2).mean()\n",
    "    \n",
    "    return residual_loss + boundary_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_data(num_samples):\n",
    "    #num_samples = 10000\n",
    "    t_samples = T * torch.rand(num_samples, dtype=Proj_dtype, device = Proj_device, requires_grad=True)\n",
    "    x_ends = torch.tensor([-3,3], dtype = Proj_dtype)\n",
    "    x_samples = x_ends[0] + (x_ends[1]- x_ends[0]) * torch.rand(num_samples , 2, dtype=Proj_dtype, device = Proj_device, requires_grad=True)\n",
    "    return t_samples,x_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define matrices for LQR problem\n",
    "\n",
    "H = torch.tensor([[1.2, 0.8], [-0.6, 0.9]], dtype=Proj_dtype, device = Proj_device)\n",
    "M = torch.tensor([[0.5,0.7], [0.3,1.0]], dtype=Proj_dtype, device = Proj_device)\n",
    "sigma = torch.tensor([[[0.08],[0.11]]], dtype=Proj_dtype, device = Proj_device)\n",
    "alpha = torch.tensor([[[1],[1]]], dtype=Proj_dtype, device = Proj_device)\n",
    "C = torch.tensor([[1.6, 0.0], [0.0, 1.1]], dtype=Proj_dtype, device = Proj_device)\n",
    "D = torch.tensor([[0.5, 0.0], [0.0, 0.7]], dtype=Proj_dtype, device = Proj_device)\n",
    "R = torch.tensor([[0.9, 0.0], [0.0, 1.0]], dtype=Proj_dtype, device = Proj_device)\n",
    "T = torch.tensor(1.0, dtype=Proj_dtype, device = Proj_device)\n",
    "\n",
    "solver = LQRSolver(H, M, sigma, C, D, R, T=T, method=\"euler\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/5\n",
      "\n",
      "Epoch 1/100 \t Loss: 158.15209147224454\n",
      "Epoch 5/100 \t Loss: 87.62574294569565\n",
      "Epoch 10/100 \t Loss: 49.41909996728251\n",
      "Epoch 15/100 \t Loss: 37.939673845038186\n",
      "Epoch 20/100 \t Loss: 33.003923233109106\n",
      "Epoch 25/100 \t Loss: 30.36266420904794\n",
      "Epoch 30/100 \t Loss: 29.37974029015094\n",
      "Epoch 35/100 \t Loss: 28.71237033580825\n",
      "Epoch 40/100 \t Loss: 28.354826613375174\n",
      "Epoch 45/100 \t Loss: 27.986046474304594\n",
      "Epoch 50/100 \t Loss: 27.75215596250623\n",
      "Epoch 55/100 \t Loss: 27.678528613434686\n",
      "Epoch 60/100 \t Loss: 27.60964817811614\n",
      "Epoch 65/100 \t Loss: 27.546015611409853\n",
      "Epoch 70/100 \t Loss: 27.517403580664556\n",
      "Epoch 75/100 \t Loss: 27.524985587174132\n",
      "Epoch 80/100 \t Loss: 27.51666185973881\n",
      "Epoch 85/100 \t Loss: 27.5106306775058\n",
      "Epoch 90/100 \t Loss: 27.50703764220195\n",
      "Epoch 95/100 \t Loss: 27.5017513333675\n",
      "Epoch 100/100 \t Loss: 27.500476665684374\n",
      "\n",
      "\n",
      "Batch 2/5\n",
      "\n",
      "Epoch 1/100 \t Loss: 36.57270957036067\n",
      "Epoch 5/100 \t Loss: 36.57119730906974\n",
      "Epoch 10/100 \t Loss: 36.56939568767366\n",
      "Epoch 15/100 \t Loss: 36.6075776753189\n",
      "Epoch 20/100 \t Loss: 36.60679462521166\n",
      "Epoch 25/100 \t Loss: 36.60632509753782\n",
      "Epoch 30/100 \t Loss: 36.606048888354806\n",
      "Epoch 35/100 \t Loss: 36.60588781042989\n",
      "Epoch 40/100 \t Loss: 36.605794213109434\n",
      "Epoch 45/100 \t Loss: 36.60573988366421\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 44\u001b[0m\n\u001b[0;32m     42\u001b[0m x_v \u001b[38;5;241m=\u001b[39m V(x_data_,requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     43\u001b[0m loss \u001b[38;5;241m=\u001b[39m total_residual(model_DGM, t_v, x_v) \n\u001b[1;32m---> 44\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m#loss.backward(retain_graph=True)\u001b[39;00m\n\u001b[0;32m     46\u001b[0m optimizer_DGM\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\warre\\Documents\\SCDAA-1\\.venv\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\warre\\Documents\\SCDAA-1\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "samplesize = [100,1000,5000]\n",
    "\n",
    "\n",
    "for sz in samplesize:\n",
    "\n",
    "\n",
    "    model_DGM = DGMNN2().double()\n",
    "    # stat_dict = torch.load('model2_DGM_state_dict.pt', map_location=torch.device('cpu'))\n",
    "    # model_DGM.load_state_dict(stat_dict)\n",
    "    #model_DGM = DGMNN().float().to(Proj_device)\n",
    "    # Prepare for training\n",
    "    optimizer_DGM = torch.optim.Adam(model_DGM.parameters(), lr=0.01)\n",
    "    scheduler_DGM = lr_scheduler.ExponentialLR(optimizer_DGM, gamma=0.9)\n",
    "\n",
    "\n",
    "    epoch_losses = []\n",
    "    # 新建文本文件,f'epochloss{%d}'sz\n",
    "    # 新建文本文件用于存储epoch的损失\n",
    "    loss_filename = f'epoch_loss_{sz}.txt'\n",
    "    with open(loss_filename, 'w') as file:\n",
    "        file.write('Epoch,Loss\\n')\n",
    "\n",
    "    Batch_size = 5\n",
    "    epochs = 40\n",
    "\n",
    "    for batch in range(Batch_size):\n",
    "    \n",
    "        print(f'Batch {batch+1}/{Batch_size}'+'\\n')\n",
    "        \n",
    "        t_data,x_data = new_data(sz)\n",
    "        dataset = TensorDataset(t_data,x_data)\n",
    "        dataloader = DataLoader(dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            model_DGM.train()\n",
    "            total_loss = 0\n",
    "            \n",
    "            for batch_idx, (t_data_,x_data_) in enumerate(dataloader):\n",
    "                optimizer_DGM.zero_grad()\n",
    "                t_v = V(t_data_,requires_grad=True)\n",
    "                x_v = V(x_data_,requires_grad=True)\n",
    "                loss = total_residual(model_DGM, t_v, x_v) \n",
    "                loss.backward(retain_graph=False)\n",
    "                #loss.backward(retain_graph=True)\n",
    "                optimizer_DGM.step()\n",
    "                total_loss += loss.item()\n",
    "            epoch_losses.append(total_loss / len(dataloader))\n",
    "            \n",
    "            scheduler_DGM.step()\n",
    "            if epoch == 0:\n",
    "                print(f'Epoch {epoch+1}/{epochs} \\t Loss: {total_loss / len(dataloader)}')\n",
    "            if(epoch+1)% 10 == 0:\n",
    "                torch.save(model_DGM.state_dict(), f'model2_DGM_state_dict_{sz}.pt')\n",
    "            if (epoch+1) % 5 == 0:\n",
    "                print(f'Epoch {epoch+1}/{epochs} \\t Loss: {total_loss / len(dataloader)}')\n",
    "\n",
    "        print('\\n')\n",
    "        # sdhsl\n",
    "        # 在每个批次结束后，以追加模式将epoch损失数据添加到文件\n",
    "        with open(loss_filename, 'a') as file:\n",
    "            for i, loss in enumerate(epoch_losses, 1):\n",
    "                file.write(f'{i},{loss}\\n')\n",
    "    model_DGM.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DGMNN2(\n",
       "  (layer1): Linear(in_features=3, out_features=100, bias=True)\n",
       "  (layer2): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (layer3): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (tanh): Tanh()\n",
       "  (relu): ReLU()\n",
       "  (output): Linear(in_features=100, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_DGM = DGMNN2().double()\n",
    "stat_dict = torch.load('model2_DGM_state_dict_5000.pt', map_location=torch.device('cpu'))\n",
    "model_DGM.load_state_dict(stat_dict)\n",
    "model_DGM.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_interval_setting = torch.load('Exercise3_MC/value_numerical/3x3/'+'interval_setting.pt')\n",
    "t_ends = load_interval_setting['t_ends']\n",
    "t_num = load_interval_setting['t_num']\n",
    "x_ends = load_interval_setting['x_ends']\n",
    "x_num = load_interval_setting['x_num']\n",
    "\n",
    "# Establish meshgrid structure from setting.\n",
    "\n",
    "t_batch_i = torch.linspace(t_ends[0],t_ends[1],t_num,dtype=torch.double)\n",
    "t_batch = t_batch_i.repeat_interleave(x_num[0]*x_num[1])\n",
    "\n",
    "x1 = torch.linspace(x_ends[0][0],x_ends[0][1],x_num[0],dtype=torch.double)\n",
    "x2 = torch.linspace(x_ends[1][0],x_ends[1][1],x_num[1],dtype=torch.double)\n",
    "\n",
    "x_batch = torch.cartesian_prod(x1, x2).unsqueeze(1).repeat(t_num, 1, 1)\n",
    "\n",
    "x_batch_i = torch.cartesian_prod(x1, x2).unsqueeze(1)\n",
    "\n",
    "X1 = x_batch_i[:, 0, 0].view(x_num[0], x_num[1])\n",
    "X2 = x_batch_i[:, 0, 1].view(x_num[0], x_num[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path_MC_FSS = 'Exercise3_MC'+ '/' + f'value_MC/{x_num[0]}x{x_num[1]}/FSS_1e5'\n",
    "\n",
    "FSS_VTSN = [int(x) for x in[5e3]]\n",
    "\n",
    "MSE_FSS_VTSN = []\n",
    "J = []\n",
    "for i in FSS_VTSN:\n",
    "    if i == 1:\n",
    "        trvlthg = ''\n",
    "    else:\n",
    "        trvlthg = 's'\n",
    "    path_FSS_VTSN_i = f\"{file_path_MC_FSS}/{i}_step{trvlthg}\"\n",
    "\n",
    "    J_load_test = torch.load(path_FSS_VTSN_i + '/value_MC.pt')\n",
    "    \n",
    "    J_processed,_ = torch.min(J_load_test,dim = 1)\n",
    "\n",
    "    J.append(J_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1.3017e+04, 2.3263e+03, 5.1811e+02, 4.4414e+03, 3.0043e+00, 4.4438e+03,\n",
       "         5.1807e+02, 2.3284e+03, 1.3023e+04, 6.4233e+03, 1.1970e+03, 2.8020e+02,\n",
       "         2.1580e+03, 1.9030e+00, 2.1573e+03, 2.8078e+02, 1.1973e+03, 6.4229e+03,\n",
       "         3.1621e+03, 6.2128e+02, 1.6471e+02, 1.0424e+03, 1.2883e+00, 1.0420e+03,\n",
       "         1.6474e+02, 6.2202e+02, 3.1622e+03, 1.5508e+03, 3.2614e+02, 1.0586e+02,\n",
       "         5.0345e+02, 9.3517e-01, 5.0310e+02, 1.0581e+02, 3.2611e+02, 1.5509e+03,\n",
       "         7.5768e+02, 1.7330e+02, 7.3826e+01, 2.4318e+02, 7.0335e-01, 2.4313e+02,\n",
       "         7.3837e+01, 1.7329e+02, 7.5778e+02, 3.6838e+02, 9.3428e+01, 5.4422e+01,\n",
       "         1.1848e+02, 5.2839e-01, 1.1849e+02, 5.4438e+01, 9.3441e+01, 3.6838e+02,\n",
       "         1.7743e+02, 5.0974e+01, 4.1271e+01, 5.8761e+01, 3.8132e-01, 5.8735e+01,\n",
       "         4.1279e+01, 5.0971e+01, 1.7738e+02, 8.4289e+01, 2.8008e+01, 3.1423e+01,\n",
       "         3.0081e+01, 2.4854e-01, 3.0077e+01, 3.1427e+01, 2.8018e+01, 8.4262e+01,\n",
       "         3.9005e+01, 1.5326e+01, 2.3572e+01, 1.6088e+01, 1.2278e-01, 1.6085e+01,\n",
       "         2.3573e+01, 1.5324e+01, 3.9017e+01], dtype=torch.float64)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DGMNN' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel_DGM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\warre\\Documents\\SCDAA-1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DGMNN' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12.6758, 13.5872,  6.5018, 16.9937,  8.9648,  9.4376, 13.2076, 10.8356,\n",
       "         6.3437, 11.8149, 12.7126,  6.3833, 15.6533,  8.4713,  9.0772, 12.0247,\n",
       "        10.0317,  6.0313, 10.9536, 11.8555,  6.1758, 14.2685,  8.0615,  8.7302,\n",
       "        10.7794,  9.2463,  5.6596, 10.0726, 10.9751,  5.8491, 12.9659,  7.6795,\n",
       "         8.4204,  9.5808,  8.5274,  5.2712,  9.0934, 10.1261,  5.4429, 11.6663,\n",
       "         7.3249,  8.0785,  8.3503,  7.8867,  4.9247,  8.0682,  9.2537,  4.9394,\n",
       "        10.3390,  7.0110,  7.6229,  7.2063,  7.2879,  4.5727,  7.0748,  8.3570,\n",
       "         4.4035,  9.0457,  6.7364,  7.1673,  6.1127,  6.6515,  4.1690,  6.1062,\n",
       "         7.4421,  3.8397,  7.8730,  6.4482,  6.6214,  5.0208,  6.0929,  3.8032,\n",
       "         5.1497,  6.4806,  3.2508,  6.7754,  6.1572,  6.0658,  4.0177,  5.4764,\n",
       "         3.4674], dtype=torch.float64, grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_data = t_batch\n",
    "x_data = x_batch.squeeze(1)\n",
    "model_DGM(torch.cat((t_data.unsqueeze(1), x_data),dim=1)).squeeze()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
