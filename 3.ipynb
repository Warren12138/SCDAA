{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "# 模型搭建\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, NN): # NL n个l（线性，全连接）隐藏层， NN 输入数据的维数， 128 256\n",
    "        # NL是有多少层隐藏层\n",
    "        # NN是每层的神经元数量\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.input_layer = nn.Linear(2, NN)\n",
    "        self.hidden_layer1 = nn.Linear(NN,int(NN/2)) ## 原文这里用NN，我这里用的下采样，经过实验验证，“等采样”更优\n",
    "        self.hidden_layer2 = nn.Linear(int(NN/2), int(NN/2))  ## 原文这里用NN，我这里用的下采样，经过实验验证，“等采样”更优\n",
    "        self.output_layer = nn.Linear(int(NN/2), 1)\n",
    "\n",
    "    def forward(self, x): # 一种特殊的方法 __call__() 回调\n",
    "        out = torch.tanh(self.input_layer(x))\n",
    "        out = torch.tanh(self.hidden_layer1(out))\n",
    "        out = torch.tanh(self.hidden_layer2(out))\n",
    "        out_final = self.output_layer(out)\n",
    "        return out_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pde(x, net):\n",
    "    u = net(x)\n",
    "    u_tx = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(net(x)),\n",
    "                               create_graph=True, allow_unused=True)[0]  # 求偏导数\n",
    "    u_t = u_tx[:, 0].unsqueeze(-1)\n",
    "    u_x = u_tx[:, 1].unsqueeze(-1)\n",
    "    u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x),\n",
    "                               create_graph=True, allow_unused=True)[0][:,1].unsqueeze(-1)  # 求偏导数\n",
    "\n",
    "    H = torch.tensor([[1.2, 0.8], [-0.6, 0.9]],dtype=torch.float32)\n",
    "    M = torch.tensor([[0.5,0.7], [0.3,1.0]],dtype=torch.float32)\n",
    "    sigma = torch.tensor([[0.8],[1.1]],dtype=torch.float32) \n",
    "    C = torch.tensor([[1.6, 0.0], [0.0, 1.1]],dtype=torch.float32)  # Positive semi-definite\n",
    "    D = torch.tensor([[0.5, 0.0], [0.0, 0.7]],dtype=torch.float32)  # Positive definite\n",
    "    R = torch.tensor([[0.9, 0.0], [0.0, 1.0]],dtype=torch.float32)  # Positive semi-definite\n",
    "    alpha = torch.tensor([[1], [1]],dtype =torch.float32)\n",
    "    alphat = torch.tensor([1,1],dtype=torch.float32)\n",
    "    realx = x[:, 1:]\n",
    "\n",
    "    return     u_t.squeeze() + 0.5 * (sigma @ sigma.transpose(0,1) @ u_xx).transpose(0,1).squeeze() + u_x.transpose(0,1) @ H @ realx + u_x.transpose(0,1) @ M @ alpha + realx.transpose(0,1) @ C @ realx + alphat @ D @ alpha\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([2000, 1])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Traning Loss: tensor(5.2837)\n",
      "100 Traning Loss: tensor(6.4104)\n",
      "200 Traning Loss: tensor(1.9076)\n",
      "300 Traning Loss: tensor(3.1256)\n",
      "400 Traning Loss: tensor(0.0700)\n",
      "500 Traning Loss: tensor(0.0623)\n",
      "600 Traning Loss: tensor(1.6365)\n",
      "700 Traning Loss: tensor(0.6291)\n",
      "800 Traning Loss: tensor(0.7593)\n",
      "900 Traning Loss: tensor(0.2838)\n",
      "1000 Traning Loss: tensor(1.7971)\n",
      "1100 Traning Loss: tensor(0.3762)\n",
      "1200 Traning Loss: tensor(0.0904)\n",
      "1300 Traning Loss: tensor(0.5712)\n",
      "1400 Traning Loss: tensor(2.3536)\n",
      "1500 Traning Loss: tensor(0.8637)\n",
      "1600 Traning Loss: tensor(1.2382)\n",
      "1700 Traning Loss: tensor(0.0590)\n",
      "1800 Traning Loss: tensor(0.4951)\n",
      "1900 Traning Loss: tensor(1.3279)\n",
      "2000 Traning Loss: tensor(0.7408)\n",
      "2100 Traning Loss: tensor(1.7497)\n",
      "2200 Traning Loss: tensor(0.4769)\n",
      "2300 Traning Loss: tensor(0.2073)\n",
      "2400 Traning Loss: tensor(0.2072)\n",
      "2500 Traning Loss: tensor(0.1078)\n",
      "2600 Traning Loss: tensor(0.9364)\n",
      "2700 Traning Loss: tensor(0.3927)\n",
      "2800 Traning Loss: tensor(1.1364)\n",
      "2900 Traning Loss: tensor(0.3258)\n",
      "3000 Traning Loss: tensor(0.7210)\n",
      "3100 Traning Loss: tensor(0.3206)\n",
      "3200 Traning Loss: tensor(1.3653)\n",
      "3300 Traning Loss: tensor(0.7038)\n",
      "3400 Traning Loss: tensor(0.2349)\n",
      "3500 Traning Loss: tensor(0.4199)\n",
      "3600 Traning Loss: tensor(0.1450)\n",
      "3700 Traning Loss: tensor(0.3360)\n",
      "3800 Traning Loss: tensor(1.3702)\n",
      "3900 Traning Loss: tensor(0.1558)\n",
      "4000 Traning Loss: tensor(0.0928)\n",
      "4100 Traning Loss: tensor(0.3998)\n",
      "4200 Traning Loss: tensor(0.6693)\n",
      "4300 Traning Loss: tensor(1.8328)\n",
      "4400 Traning Loss: tensor(0.4810)\n",
      "4500 Traning Loss: tensor(0.6321)\n",
      "4600 Traning Loss: tensor(0.4494)\n",
      "4700 Traning Loss: tensor(0.6567)\n",
      "4800 Traning Loss: tensor(0.2259)\n",
      "4900 Traning Loss: tensor(0.1857)\n",
      "5000 Traning Loss: tensor(0.8582)\n",
      "5100 Traning Loss: tensor(0.4334)\n",
      "5200 Traning Loss: tensor(0.5495)\n",
      "5300 Traning Loss: tensor(1.5777)\n",
      "5400 Traning Loss: tensor(0.7035)\n",
      "5500 Traning Loss: tensor(0.0575)\n",
      "5600 Traning Loss: tensor(0.9460)\n",
      "5700 Traning Loss: tensor(0.5867)\n",
      "5800 Traning Loss: tensor(0.3536)\n",
      "5900 Traning Loss: tensor(0.2824)\n",
      "6000 Traning Loss: tensor(0.9282)\n",
      "6100 Traning Loss: tensor(1.7377)\n",
      "6200 Traning Loss: tensor(1.1709)\n",
      "6300 Traning Loss: tensor(0.5050)\n",
      "6400 Traning Loss: tensor(0.9761)\n",
      "6500 Traning Loss: tensor(0.0309)\n",
      "6600 Traning Loss: tensor(1.8013)\n",
      "6700 Traning Loss: tensor(0.2832)\n",
      "6800 Traning Loss: tensor(0.0208)\n",
      "6900 Traning Loss: tensor(0.3412)\n",
      "7000 Traning Loss: tensor(0.0092)\n",
      "7100 Traning Loss: tensor(0.5076)\n",
      "7200 Traning Loss: tensor(0.9297)\n",
      "7300 Traning Loss: tensor(0.3502)\n",
      "7400 Traning Loss: tensor(1.6384)\n",
      "7500 Traning Loss: tensor(0.6749)\n",
      "7600 Traning Loss: tensor(0.1090)\n",
      "7700 Traning Loss: tensor(0.5796)\n",
      "7800 Traning Loss: tensor(0.1293)\n",
      "7900 Traning Loss: tensor(0.1104)\n",
      "8000 Traning Loss: tensor(1.0967)\n",
      "8100 Traning Loss: tensor(0.4907)\n",
      "8200 Traning Loss: tensor(0.1434)\n",
      "8300 Traning Loss: tensor(0.4610)\n",
      "8400 Traning Loss: tensor(0.4613)\n",
      "8500 Traning Loss: tensor(0.2355)\n",
      "8600 Traning Loss: tensor(0.3255)\n",
      "8700 Traning Loss: tensor(0.5931)\n",
      "8800 Traning Loss: tensor(0.2210)\n",
      "8900 Traning Loss: tensor(0.6422)\n",
      "9000 Traning Loss: tensor(0.5170)\n",
      "9100 Traning Loss: tensor(0.6971)\n",
      "9200 Traning Loss: tensor(0.8629)\n",
      "9300 Traning Loss: tensor(0.0482)\n",
      "9400 Traning Loss: tensor(0.1219)\n",
      "9500 Traning Loss: tensor(0.4534)\n",
      "9600 Traning Loss: tensor(0.0881)\n",
      "9700 Traning Loss: tensor(0.5883)\n",
      "9800 Traning Loss: tensor(0.0864)\n",
      "9900 Traning Loss: tensor(0.4482)\n"
     ]
    }
   ],
   "source": [
    "net = Net(30)\n",
    "mse_cost_function = torch.nn.MSELoss(reduction='mean')  # Mean squared error\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-4)\n",
    "\n",
    "t_bc_zeros = np.zeros((2, 1))\n",
    "x_in_pos_one = np.ones((2, 1))\n",
    "x_in_neg_one = -np.ones((2, 1))\n",
    "u_in_zeros = np.zeros((2, 1))\n",
    "\n",
    "R = np.array([[0.9, 0.0], [0.0, 1.0]] )\n",
    "\n",
    "iterations = 10000\n",
    "for epoch in range(iterations):\n",
    "    optimizer.zero_grad()  # 梯度归0\n",
    "\n",
    "    t_in_var = np.random.uniform(low=0, high=1.0, size=(2, 1))\n",
    "    x_bc_var = np.random.uniform(low=-1.0, high=1.0, size=(2, 1))\n",
    "    u_bc_sin = x_bc_var.transpose() @ R @ x_bc_var\n",
    "\n",
    "    pt_x_bc_var = Variable(torch.from_numpy(x_bc_var).float(), requires_grad=False)\n",
    "    pt_t_bc_zeros = Variable(torch.from_numpy(t_bc_zeros).float(), requires_grad=False)\n",
    "    pt_u_bc_sin = Variable(torch.from_numpy(u_bc_sin).float(), requires_grad=False)\n",
    "    pt_x_in_pos_one = Variable(torch.from_numpy(x_in_pos_one).float(), requires_grad=False)\n",
    "    pt_x_in_neg_one = Variable(torch.from_numpy(x_in_neg_one).float(), requires_grad=False)\n",
    "    pt_t_in_var = Variable(torch.from_numpy(t_in_var).float(), requires_grad=False)\n",
    "    pt_u_in_zeros = Variable(torch.from_numpy(u_in_zeros).float(), requires_grad=False)\n",
    "\n",
    "    net_bc_out = net(torch.cat([pt_t_bc_zeros, pt_x_bc_var], 1))  # u(x,t)的输出\n",
    "    mse_u_2 = mse_cost_function(net_bc_out, pt_u_bc_sin)  # e = u(x,t)-(-sin(pi*x))  公式（2）\n",
    "\n",
    "\n",
    "    x_collocation = np.random.uniform(low=-1.0, high=1.0, size=(2, 1))\n",
    "    t_collocation = np.random.uniform(low=0.0, high=1.0, size=(2, 1))\n",
    "    all_zeros = np.zeros((2000, 1))\n",
    "    pt_x_collocation = Variable(torch.from_numpy(x_collocation).float(), requires_grad=True)\n",
    "    pt_t_collocation = Variable(torch.from_numpy(t_collocation).float(), requires_grad=True)\n",
    "    pt_all_zeros = Variable(torch.from_numpy(all_zeros).float(), requires_grad=False)\n",
    "\n",
    "     # 确保pt_t_collocation和pt_x_collocation有相同的数据类型\n",
    "    pt_t_collocation = pt_t_collocation.to(torch.float32)\n",
    "    pt_x_collocation = pt_x_collocation.to(torch.float32)\n",
    "\n",
    "    # 将变量x,t带入公式（1）\n",
    "    f_out = pde(torch.cat([pt_t_collocation, pt_x_collocation], 1), net)  # output of f(x,t) 公式（1）\n",
    "    mse_f_1 = mse_cost_function(f_out, pt_all_zeros)\n",
    "\n",
    "        # 将误差(损失)累加起来\n",
    "    loss = mse_f_1 + mse_u_2 \n",
    "\n",
    "    loss.backward()  # 反向传播\n",
    "    optimizer.step()  # This is equivalent to : theta_new = theta_old - alpha * derivative of J w.r.t theta\n",
    "\n",
    "    with torch.autograd.no_grad():\n",
    "        if epoch % 100 == 0:\n",
    "            print(epoch, \"Traning Loss:\", loss.data)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
