{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable as V\n",
    "from lib.Exercise1_1 import LQRSolver\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import time \n",
    "\n",
    "Proj_dtype = torch.double\n",
    "Proj_device = 'cpu'\n",
    "\n",
    "class DGMNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DGMNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(3, 100)  \n",
    "        self.layer2 = nn.Linear(100, 100)\n",
    "        self.layer3 = nn.Linear(100, 100)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(100, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.tanh(self.layer2(x))\n",
    "        x = self.relu(self.layer3(x))\n",
    "\n",
    "        return self.output(x)\n",
    "    \n",
    "# Assuming the use of the provided DGMNN class for the value function approximation\n",
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"Neural Network for approximating the policy.\"\"\"\n",
    "    def __init__(self):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(3, 100)  \n",
    "        self.layer2 = nn.Linear(100, 100)\n",
    "        self.layer3 = nn.Linear(100, 100)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(100, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.tanh(self.layer2(x))\n",
    "        x = self.relu(self.layer3(x))\n",
    "\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define matrices for LQR problem\n",
    "\n",
    "H = torch.tensor([[1.2, 0.8], [-0.6, 0.9]], dtype=Proj_dtype, device = Proj_device)\n",
    "M = torch.tensor([[0.5,0.7], [0.3,1.0]], dtype=Proj_dtype, device = Proj_device)\n",
    "sigma = torch.tensor([[[0.08],[0.11]]], dtype=Proj_dtype, device = Proj_device)\n",
    "alpha = torch.tensor([[[1],[1]]], dtype=Proj_dtype, device = Proj_device)\n",
    "C = torch.tensor([[1.6, 0.0], [0.0, 1.1]], dtype=Proj_dtype, device = Proj_device)\n",
    "D = torch.tensor([[0.5, 0.0], [0.0, 0.7]], dtype=Proj_dtype, device = Proj_device)\n",
    "R = torch.tensor([[0.9, 0.0], [0.0, 1.0]], dtype=Proj_dtype, device = Proj_device)\n",
    "T = torch.tensor(1.0, dtype=Proj_dtype, device = Proj_device)\n",
    "\n",
    "solver = LQRSolver(H, M, sigma, C, D, R, T=T, method=\"euler\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hessian(grad,x):\n",
    "    Hessian = torch.tensor([], device = Proj_device)\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        hessian = torch.tensor([], device = Proj_device)\n",
    "        for j in range(len(grad[i])):\n",
    "            u_xxi = torch.autograd.grad(grad[i][j], x, grad_outputs=torch.ones_like(grad[i][j]), retain_graph=True,create_graph=True, allow_unused=True)[0]           \n",
    "            hessian = torch.cat((hessian, u_xxi[i].unsqueeze(0)))\n",
    "        Hessian = torch.cat((Hessian, hessian.unsqueeze(0)),dim = 0)\n",
    "        #print(Hessian)\n",
    "    return Hessian\n",
    "\n",
    "def get_hessian_(model,t,x):\n",
    "    Hessian = torch.tensor([], device = Proj_device)\n",
    "    for i in range(len(t)):\n",
    "        x_i = V(x[i],requires_grad=True)\n",
    "        input = torch.cat(((t[i]).unsqueeze(0), x_i),dim=0)\n",
    "        u_in = model(input)\n",
    "        grad = torch.autograd.grad(u_in, x_i, grad_outputs=torch.ones_like(u_in), create_graph=True, retain_graph=True)[0]\n",
    "        hessian = torch.tensor([], device = Proj_device)\n",
    "        for j in range(len(grad)):\n",
    "            u_xxi = torch.autograd.grad(grad[j], x_i, grad_outputs=torch.ones_like(grad[j]), retain_graph=True,create_graph=True, allow_unused=True)[0]           \n",
    "            hessian = torch.cat((hessian, u_xxi.unsqueeze(0)))\n",
    "        Hessian = torch.cat((Hessian, hessian.unsqueeze(0)),dim = 0)\n",
    "    return Hessian\n",
    "\n",
    "def pde_residual(value_network, policy_network, t, x):\n",
    "    # 准备输入\n",
    "    input = torch.cat((t.unsqueeze(1), x), dim=1)\n",
    "    \n",
    "    # 获取价值网络的输出\n",
    "    u = value_network(input)\n",
    "    u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), create_graph=True, retain_graph=True)[0]\n",
    "    u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    # 使用策略网络更新alpha\n",
    "    a = policy_network(input).detach()  # 防止alpha的梯度传播\n",
    "    # Perform matrix multiplication correctly, assuming M is [2, 2] and a is [100, 2]\n",
    "    a_transformed = M @ a.T  # Results in a [2, 100] tensor\n",
    "    a_transformed = a_transformed.T  # Correct the shape to [100, 2] to match u_x for element-wise multiplication\n",
    "\n",
    "    # Now you can multiply u_x with a_transformed element-wise directly\n",
    "    residual_component = u_x * a_transformed  # This should not raise an error given compatible shapes\n",
    "    # 计算二阶导数\n",
    "    u_xx = get_hessian(u_x, x)\n",
    "    d_a_product = D @ a.T  # 结果形状 [2, batch_size]\n",
    "    d_a_product = d_a_product.T  # 调整形状为 [batch_size, 2] 以匹配 a_transformed\n",
    "\n",
    "    # 现在，a_transformed 和 d_a_product 形状一致，可以进行逐元素乘法\n",
    "    # 注意，如果原本的目的是将 a_transformed 和 D @ a 的结果相乘，这里假设两者均为 [batch_size, 2]\n",
    "    residual_component_1 = a_transformed * d_a_product  # 形状为 [batch_size, 2]\n",
    "    # Continue with your residual calculation\n",
    "    residual = u_t + 0.5 * torch.einsum('bii->b', sigma @ sigma.transpose(1,2) @ u_xx) + \\\n",
    "           torch.sum(residual_component, dim=1, keepdim=True) + \\\n",
    "           torch.sum(x * (C @ x.T).T, dim=1, keepdim=True) + \\\n",
    "           torch.sum(residual_component_1, dim=1, keepdim=True)  # Ensure this operation is also corrected if needed\n",
    "   \n",
    "    return residual\n",
    "\n",
    "\n",
    "def boundary_condition(model,t, x):\n",
    "\n",
    "    \n",
    "    T_input = T * torch.ones_like(t)\n",
    "\n",
    "    input = torch.cat((T_input.unsqueeze(1), x),dim=1)\n",
    "    u = model(input)\n",
    "\n",
    "    return u - (x.unsqueeze(1) @ R @ x.unsqueeze(1).transpose(1,2)).squeeze()\n",
    "\n",
    "def total_residual(model, t, x):\n",
    "    \n",
    "    residual_loss = pde_residual(model, t, x).pow(2).mean()\n",
    "    boundary_loss = boundary_condition(model,t,x).pow(2).mean()\n",
    "    \n",
    "    return residual_loss + boundary_loss\n",
    "\n",
    "\n",
    "def boundary_condition_loss(model, x_samples):\n",
    "    # 假设边界条件为模型输出在边界上应该为0\n",
    "    # 这里需要根据您的具体问题来调整\n",
    "    # x_samples 应包含边界上的样本点\n",
    "    \n",
    "    # 获取模型在边界样本点上的预测\n",
    "    boundary_predictions = model(x_samples)\n",
    "    \n",
    "    # 计算损失：例如，可以使用 MSE 损失来量化模型输出与0之间的差异\n",
    "    boundary_loss = torch.mean(boundary_predictions ** 2)\n",
    "    \n",
    "    return boundary_loss\n",
    "\n",
    "\n",
    "def compute_total_loss(value_network, policy_network, t_samples, x_samples):\n",
    "    residual = pde_residual(value_network, policy_network, t_samples, x_samples)\n",
    "    boundary_loss = boundary_condition_loss(value_network, x_samples)\n",
    "    \n",
    "    total_loss = torch.mean(residual.pow(2)) + torch.mean(boundary_loss.pow(2))\n",
    "    return total_loss\n",
    "\n",
    "def train_value_network(value_network, policy_network, t_samples, x_samples, epochs):\n",
    "    value_network.train()\n",
    "    final_loss = None\n",
    "    for epoch in range(epochs):\n",
    "        # 这里保持原样，不需要修改\n",
    "        value_optimizer.zero_grad()\n",
    "        loss = compute_total_loss(value_network, policy_network, t_samples, x_samples)  # 使用原始参数\n",
    "        loss.backward()\n",
    "        value_optimizer.step()\n",
    "        \n",
    "        if epoch == epochs - 1:\n",
    "            final_loss = loss.item()\n",
    "    return final_loss\n",
    "\n",
    "\n",
    "def policy_loss(value_network, policy_network, t, x):\n",
    "    # 计算价值网络输出对于策略网络输出的梯度\n",
    "    alpha = policy_network(torch.cat((t.unsqueeze(1), x), dim=1))\n",
    "    combined_input = torch.cat((t.unsqueeze(1), x, alpha), dim=1)\n",
    "\n",
    "    # 将合并后的输入传递给价值网络\n",
    "    value_output = value_network(combined_input)\n",
    "    \n",
    "    # 定义策略损失函数，这可能依赖于具体的任务\n",
    "    # 例如，可以是负的价值函数的输出，我们希望最大化价值\n",
    "    loss = -torch.mean(value_output)\n",
    "    return loss\n",
    "\n",
    "def update_policy(policy_network, value_optimizer, t_samples, x_samples):\n",
    "    policy_network.train()\n",
    "    policy_optimizer.zero_grad()\n",
    "    \n",
    "    loss = policy_loss(value_network, policy_network, t_samples, x_samples)\n",
    "    loss.backward()\n",
    "    \n",
    "    policy_optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_data(num_samples):\n",
    "    #num_samples = 10000\n",
    "    t_samples = T * torch.rand(num_samples, dtype=Proj_dtype, device = Proj_device, requires_grad=True)\n",
    "    x_ends = torch.tensor([-3,3], dtype = Proj_dtype)\n",
    "    x_samples = x_ends[0] + (x_ends[1]- x_ends[0]) * torch.rand(num_samples , 2, dtype=Proj_dtype, device = Proj_device, requires_grad=True)\n",
    "    return t_samples,x_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_network = DGMNN().to(device=Proj_device, dtype=Proj_dtype)\n",
    "policy_network = PolicyNetwork().to(device=Proj_device, dtype=Proj_dtype)\n",
    "\n",
    "\n",
    "# Optimizers\n",
    "value_optimizer = torch.optim.Adam(value_network.parameters(), lr=0.001)\n",
    "policy_optimizer = torch.optim.Adam(policy_network.parameters(), lr=0.001)\n",
    "\n",
    "# Generate data\n",
    "num_samples = 10  # Adjust as needed\n",
    "t_samples, x_samples = new_data(num_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (10x2 and 3x100)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m num_iterations \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iterations):\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mtrain_value_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     current_loss \u001b[38;5;241m=\u001b[39m update_policy(policy_network, value_optimizer, t_samples, x_samples)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# 收敛性检查\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[21], line 106\u001b[0m, in \u001b[0;36mtrain_value_network\u001b[1;34m(value_network, policy_network, t_samples, x_samples, epochs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;66;03m# 这里保持原样，不需要修改\u001b[39;00m\n\u001b[0;32m    105\u001b[0m     value_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 106\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_total_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_samples\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 使用原始参数\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    108\u001b[0m     value_optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[21], line 95\u001b[0m, in \u001b[0;36mcompute_total_loss\u001b[1;34m(value_network, policy_network, t_samples, x_samples)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_total_loss\u001b[39m(value_network, policy_network, t_samples, x_samples):\n\u001b[0;32m     94\u001b[0m     residual \u001b[38;5;241m=\u001b[39m pde_residual(value_network, policy_network, t_samples, x_samples)\n\u001b[1;32m---> 95\u001b[0m     boundary_loss \u001b[38;5;241m=\u001b[39m \u001b[43mboundary_condition_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(residual\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(boundary_loss\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m total_loss\n",
      "Cell \u001b[1;32mIn[21], line 85\u001b[0m, in \u001b[0;36mboundary_condition_loss\u001b[1;34m(model, x_samples)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mboundary_condition_loss\u001b[39m(model, x_samples):\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;66;03m# 假设边界条件为模型输出在边界上应该为0\u001b[39;00m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;66;03m# 这里需要根据您的具体问题来调整\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;66;03m# x_samples 应包含边界上的样本点\u001b[39;00m\n\u001b[0;32m     83\u001b[0m     \n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m# 获取模型在边界样本点上的预测\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m     boundary_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m# 计算损失：例如，可以使用 MSE 损失来量化模型输出与0之间的差异\u001b[39;00m\n\u001b[0;32m     88\u001b[0m     boundary_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(boundary_predictions \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\warre\\Documents\\SCDAA-1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\warre\\Documents\\SCDAA-1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[19], line 23\u001b[0m, in \u001b[0;36mDGMNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 23\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     24\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x))\n\u001b[0;32m     25\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x))\n",
      "File \u001b[1;32mc:\\Users\\warre\\Documents\\SCDAA-1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\warre\\Documents\\SCDAA-1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\warre\\Documents\\SCDAA-1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (10x2 and 3x100)"
     ]
    }
   ],
   "source": [
    "previous_loss = float('inf')\n",
    "loss_threshold = 1e-4\n",
    "num_iterations = 10\n",
    "for iteration in range(num_iterations):\n",
    "    train_value_network(value_network, policy_network, t_samples, x_samples, epochs=100)\n",
    "    current_loss = update_policy(policy_network, value_optimizer, t_samples, x_samples)\n",
    "    \n",
    "    # 收敛性检查\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n",
    "def compute_total_loss(value_network, policy_network, t_samples, x_samples):\n",
    "    residual = pde_residual(value_network, policy_network, t_samples, x_samples)\n",
    "    boundary_loss = boundary_condition_loss(value_network, x_samples)\n",
    "    \n",
    "    total_loss = torch.mean(residual.pow(2)) + torch.mean(boundary_loss.pow(2))\n",
    "    return total_loss\n",
    "\n",
    "def train_value_network(value_network, policy_network, t_samples, x_samples, epochs):\n",
    "    value_network.train()\n",
    "    final_loss = None\n",
    "    for epoch in range(epochs):\n",
    "        value_optimizer.zero_grad()\n",
    "        loss = compute_total_loss(value_network, policy_network, t_samples, x_samples)\n",
    "        loss.backward()\n",
    "        value_optimizer.step()\n",
    "        \n",
    "        if epoch == epochs - 1:\n",
    "            final_loss = loss.item()\n",
    "    return final_loss\n",
    "\n",
    "def compute_policy_loss(value_network, policy_network, states):\n",
    "    # Ensure states require gradients\n",
    "    states = states.requires_grad_(True)\n",
    "\n",
    "    # Obtain actions from the policy network\n",
    "    actions = policy_network(states)\n",
    "    \n",
    "    # Combine states and actions in the way they're inputted to your value network\n",
    "    # This step depends on your specific setup\n",
    "    value_network_input = torch.cat([states, actions], dim=-1)\n",
    "    \n",
    "    # Get the value predictions\n",
    "    values = value_network(value_network_input)\n",
    "    \n",
    "    # Compute gradients of values with respect to states\n",
    "    value_gradients = torch.autograd.grad(outputs=values.sum(), inputs=states, create_graph=True)[0]\n",
    "\n",
    "    # Policy loss can be formulated in several ways depending on the specific goal\n",
    "    # For example, maximizing the value (encouraging positive gradients)\n",
    "    policy_loss = -value_gradients.mean()\n",
    "    \n",
    "    return policy_loss\n",
    "\n",
    "def update_policy(policy_network, value_network, epochs, states):\n",
    "    \"\"\"Update the policy network based on the current value function.\"\"\"\n",
    "    policy_network.train()\n",
    "    for epoch in range(epochs):\n",
    "        policy_optimizer.zero_grad()\n",
    "        # Define a loss function for the policy network based on the updated value function\n",
    "        # This could involve computing gradients of the value network's outputs with respect to its inputs\n",
    "        policy_loss = policy_loss = compute_policy_loss(value_network, policy_network, states)\n",
    "        policy_loss.backward()\n",
    "        policy_optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_loss = float('inf')\n",
    "loss_threshold = 1e-4  # Define a suitable threshold for loss convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected size for first two dimensions of batch2 tensor to be: [100, 2] but got: [100, 1].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 10\u001b[0m\n\u001b[0;32m      5\u001b[0m t_samples, x_samples \u001b[38;5;241m=\u001b[39m new_data(num_samples)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iterations):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Step 1: Train the value network for the current policy\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Ensure train_value_network returns the current loss\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     current_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_value_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Check for convergence based on loss\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     loss_change \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(previous_loss \u001b[38;5;241m-\u001b[39m current_loss)\n",
      "Cell \u001b[1;32mIn[33], line 79\u001b[0m, in \u001b[0;36mtrain_value_network\u001b[1;34m(value_network, policy_network, t_samples, x_samples, epochs)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     78\u001b[0m     value_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 79\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_total_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     81\u001b[0m     value_optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[33], line 68\u001b[0m, in \u001b[0;36mcompute_total_loss\u001b[1;34m(value_network, policy_network, t_samples, x_samples)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_total_loss\u001b[39m(value_network, policy_network, t_samples, x_samples):\n\u001b[1;32m---> 68\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[43mpde_residual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m     boundary_loss \u001b[38;5;241m=\u001b[39m boundary_condition_loss(value_network, x_samples)\n\u001b[0;32m     71\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(residual\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(boundary_loss\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m))\n",
      "Cell \u001b[1;32mIn[33], line 44\u001b[0m, in \u001b[0;36mpde_residual\u001b[1;34m(value_network, policy_network, t, x)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(a\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Assuming a is [batch_size, 2]\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Direct matrix multiplication without unnecessary transposition\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m#        torch.sum(x * (C @ x.T).T, dim=1, keepdim=True) + \\\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m#        torch.sum(a_transformed * (D @ a.T), dim=1, keepdim=True)  # Ensure this operation is also corrected if needed\u001b[39;00m\n\u001b[0;32m     42\u001b[0m residual \u001b[38;5;241m=\u001b[39m u_t \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbii->b\u001b[39m\u001b[38;5;124m'\u001b[39m, sigma \u001b[38;5;241m@\u001b[39m sigma\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m@\u001b[39m u_xx) \u001b[38;5;241m+\u001b[39m \\\n\u001b[0;32m     43\u001b[0m        torch\u001b[38;5;241m.\u001b[39msum(u_x \u001b[38;5;241m*\u001b[39m (H \u001b[38;5;241m@\u001b[39m x\u001b[38;5;241m.\u001b[39mT)\u001b[38;5;241m.\u001b[39mT, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m---> 44\u001b[0m        torch\u001b[38;5;241m.\u001b[39msum(u_x \u001b[38;5;241m*\u001b[39m (\u001b[43mM\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m \\\n\u001b[0;32m     45\u001b[0m        torch\u001b[38;5;241m.\u001b[39msum(x \u001b[38;5;241m*\u001b[39m (C \u001b[38;5;241m@\u001b[39m x\u001b[38;5;241m.\u001b[39mT)\u001b[38;5;241m.\u001b[39mT, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \\\n\u001b[0;32m     46\u001b[0m        torch\u001b[38;5;241m.\u001b[39msum(a \u001b[38;5;241m*\u001b[39m (D \u001b[38;5;241m@\u001b[39m a\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected size for first two dimensions of batch2 tensor to be: [100, 2] but got: [100, 1]."
     ]
    }
   ],
   "source": [
    "num_iterations = 100\n",
    "\n",
    "num_samples = 100 \n",
    "\n",
    "t_samples, x_samples = new_data(num_samples)\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "    # Step 1: Train the value network for the current policy\n",
    "    # Ensure train_value_network returns the current loss\n",
    "    current_loss = train_value_network(value_network, policy_network, t_samples, x_samples, epochs=100)\n",
    "    \n",
    "    # Check for convergence based on loss\n",
    "    loss_change = abs(previous_loss - current_loss)\n",
    "    if loss_change < loss_threshold:\n",
    "        print(f\"Convergence achieved at iteration {iteration} with loss change {loss_change}\")\n",
    "        break\n",
    "    previous_loss = current_loss\n",
    "    \n",
    "    # Step 2: Update the policy based on the current value function\n",
    "    # Ensure update_policy function is correctly implemented\n",
    "    update_policy(policy_network, value_network, t_samples, x_samples, epochs=100)\n",
    "\n",
    "    # Optional: Print progress\n",
    "    print(f\"Iteration {iteration}, Loss Change: {loss_change}\")\n",
    "\n",
    "    # Implement additional checks as needed, e.g., based on policy changes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
