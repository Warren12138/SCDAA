{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "from lib.Exercise1_1 import LQRSolver\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "Proj_dtype = torch.double\n",
    "Proj_device = 'cpu' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGMhiddenlayerYYBver(nn.Module):\n",
    "\n",
    "    # From the original paper of Justin's, presented by Yuebo Yang Apr.9th 2024\n",
    "    \n",
    "    def __init__(self, input_f, output_f, activation = 'tanh'):\n",
    "        \n",
    "        super(DGMhiddenlayerYYBver, self).__init__()\n",
    "\n",
    "        self.input_f = input_f\n",
    "        self.output_f = output_f\n",
    "\n",
    "        # Params\n",
    "\n",
    "        # Zl's\n",
    "\n",
    "        self.Uzl = nn.Parameter(torch.Tensor(output_f, input_f))\n",
    "        self.Wzl = nn.Parameter(torch.Tensor(output_f, output_f))\n",
    "        self.bzl = nn.Parameter(torch.Tensor(output_f))\n",
    "\n",
    "        # Gl's\n",
    "\n",
    "        self.Ugl = nn.Parameter(torch.Tensor(output_f, input_f))\n",
    "        self.Wgl = nn.Parameter(torch.Tensor(output_f, output_f))\n",
    "        self.bgl = nn.Parameter(torch.Tensor(output_f))\n",
    "\n",
    "        # Rl's\n",
    "\n",
    "        self.Url = nn.Parameter(torch.Tensor(output_f, input_f))\n",
    "        self.Wrl = nn.Parameter(torch.Tensor(output_f, output_f))\n",
    "        self.brl = nn.Parameter(torch.Tensor(output_f))\n",
    "\n",
    "        # Hl's\n",
    "\n",
    "        self.Uhl = nn.Parameter(torch.Tensor(output_f, input_f))\n",
    "        self.Whl = nn.Parameter(torch.Tensor(output_f, output_f))\n",
    "        self.bhl = nn.Parameter(torch.Tensor(output_f))\n",
    "\n",
    "\n",
    "        if activation == 'tanh':\n",
    "            self.activation = torch.tanh\n",
    "        else:\n",
    "            self.activation = None \n",
    "\n",
    "        self.init_method = 'normal' # or 'uniform'\n",
    "\n",
    "        self._initialize_params()\n",
    "\n",
    "    def _initialize_params(self):\n",
    "\n",
    "        if self.init_method == 'uniform':\n",
    "            for param in self.parameters():\n",
    "                if param.dim() > 1:\n",
    "                    init.xavier_uniform_(param)  \n",
    "                else:\n",
    "                    init.constant_(param, 0) \n",
    "                    \n",
    "        if self.init_method == 'normal':\n",
    "            for param in self.parameters():\n",
    "                if param.dim() > 1:\n",
    "                    init.xavier_normal_(param)  \n",
    "                else:\n",
    "                    init.constant_(param, 0) \n",
    "\n",
    "    def forward(self, x, S1, Sl):\n",
    "\n",
    "        Zl = self.activation(torch.mm(x, self.Uzl.t())+ torch.mm(Sl, self.Wzl.t()) + self.bzl)\n",
    "\n",
    "        Gl = self.activation(torch.mm(x, self.Ugl.t())+ torch.mm(S1, self.Wgl.t()) + self.bgl)\n",
    "\n",
    "        Rl = self.activation(torch.mm(x, self.Url.t())+ torch.mm(Sl, self.Wrl.t()) + self.brl)\n",
    "\n",
    "        Hl = self.activation(torch.mm(x, self.Uhl.t())+ torch.mm(torch.mul(Sl,Rl), self.Whl.t()) + self.bhl)\n",
    "\n",
    "        Sl_1 = torch.mul((1-Gl),Hl) + torch.mul(Zl,Sl)\n",
    "\n",
    "        return Sl_1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DGMNN_YYBver(nn.Module):\n",
    "\n",
    "    # From the original paper of Justin's, presented by Yuebo Yang Apr.9th 2024\n",
    "\n",
    "    def __init__(self, init_method = 'uniform'):\n",
    "        super(DGMNN_YYBver, self).__init__()\n",
    "\n",
    "        self.nodenum = 50\n",
    "\n",
    "        self.layer1 = DGMhiddenlayerYYBver(3, self.nodenum)\n",
    "        self.layer2 = DGMhiddenlayerYYBver(3, self.nodenum)\n",
    "        self.layer3 = DGMhiddenlayerYYBver(3, self.nodenum)\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # Params\n",
    "\n",
    "        # S1's\n",
    "\n",
    "        self.W1 = nn.Parameter(torch.Tensor(self.nodenum, 3))\n",
    "        self.b1 = nn.Parameter(torch.Tensor(self.nodenum))\n",
    "\n",
    "        # Output's\n",
    "\n",
    "        self.W = nn.Parameter(torch.Tensor(1, self.nodenum))\n",
    "        self.b = nn.Parameter(torch.Tensor(1))\n",
    "\n",
    "        self.activation = torch.tanh\n",
    "\n",
    "        self.init_method = 'normal' # or 'uniform'\n",
    "\n",
    "        self._initialize_params()\n",
    "\n",
    "    def _initialize_params(self):\n",
    "\n",
    "        if self.init_method == 'uniform':\n",
    "            for param in self.parameters():\n",
    "                if param.dim() > 1:\n",
    "                    init.xavier_uniform_(param)  \n",
    "                else:\n",
    "                    init.constant_(param, 0) \n",
    "                    \n",
    "        if self.init_method == 'normal':\n",
    "            for param in self.parameters():\n",
    "                if param.dim() > 1:\n",
    "                    init.xavier_normal_(param)  \n",
    "                else:\n",
    "                    init.constant_(param, 0) \n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        S_1 = self.activation(torch.mm(x, self.W1.t()) + self.b1)\n",
    "        # l=1\n",
    "        S_2 = self.layer1(x,S_1,S_1)\n",
    "        # l=2\n",
    "        S_3 = self.layer2(x,S_1,S_2)\n",
    "        # l=3\n",
    "        S_4 = self.layer3(x,S_1,S_3)\n",
    "\n",
    "        output = torch.mm(S_4, self.W.t()) + self.b\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ControlNN_YYBver(nn.Module):\n",
    "\n",
    "    # From the original paper of Justin's, presented by Yuebo Yang Apr.9th 2024\n",
    "\n",
    "    def __init__(self, init_method = 'uniform'):\n",
    "        super(ControlNN_YYBver, self).__init__()\n",
    "\n",
    "        self.nodenum = 50\n",
    "\n",
    "        self.layer1 = DGMhiddenlayerYYBver(3, self.nodenum)\n",
    "        self.layer2 = DGMhiddenlayerYYBver(3, self.nodenum)\n",
    "        self.layer3 = DGMhiddenlayerYYBver(3, self.nodenum)\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # Params\n",
    "\n",
    "        # S1's\n",
    "\n",
    "        self.W1 = nn.Parameter(torch.Tensor(self.nodenum, 3))\n",
    "        self.b1 = nn.Parameter(torch.Tensor(self.nodenum))\n",
    "\n",
    "        # Output's\n",
    "\n",
    "        self.W = nn.Parameter(torch.Tensor(2, self.nodenum))\n",
    "        self.b = nn.Parameter(torch.Tensor(2))\n",
    "\n",
    "        self.activation = torch.tanh\n",
    "\n",
    "        self.init_method = 'normal' # or 'uniform'\n",
    "\n",
    "        self._initialize_params()\n",
    "\n",
    "    def _initialize_params(self):\n",
    "\n",
    "        if self.init_method == 'uniform':\n",
    "            for param in self.parameters():\n",
    "                if param.dim() > 1:\n",
    "                    init.xavier_uniform_(param)  \n",
    "                else:\n",
    "                    init.constant_(param, 0) \n",
    "                    \n",
    "        if self.init_method == 'normal':\n",
    "            for param in self.parameters():\n",
    "                if param.dim() > 1:\n",
    "                    init.xavier_normal_(param)  \n",
    "                else:\n",
    "                    init.constant_(param, 0) \n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        S_1 = self.activation(torch.mm(x, self.W1.t()) + self.b1)\n",
    "        # l=1\n",
    "        S_2 = self.layer1(x,S_1,S_1)\n",
    "        # l=2\n",
    "        S_3 = self.layer2(x,S_1,S_2)\n",
    "        # l=3\n",
    "        S_4 = self.layer3(x,S_1,S_3)\n",
    "\n",
    "        output = torch.mm(S_4, self.W.t()) + self.b\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hessian(grad,x):\n",
    "    Hessian = torch.tensor([], device = Proj_device)\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        hessian = torch.tensor([], device = Proj_device)\n",
    "        for j in range(len(grad[i])):\n",
    "            u_xxi = torch.autograd.grad(grad[i][j], x, grad_outputs=torch.ones_like(grad[i][j]), retain_graph=True,create_graph=True, allow_unused=True)[0]           \n",
    "            hessian = torch.cat((hessian, u_xxi[i].unsqueeze(0)))\n",
    "        Hessian = torch.cat((Hessian, hessian.unsqueeze(0)),dim = 0)\n",
    "        # print(Hessian)\n",
    "    return Hessian\n",
    "\n",
    "def get_hessian_(model,t,x):\n",
    "    Hessian = torch.tensor([], device = Proj_device)\n",
    "    for i in range(len(t)):\n",
    "        x_i = V(x[i],requires_grad=True)\n",
    "        input = torch.cat(((t[i]).unsqueeze(0), x_i),dim=0)\n",
    "        u_in = model(input)\n",
    "        grad = torch.autograd.grad(u_in, x_i, grad_outputs=torch.ones_like(u_in), create_graph=True, retain_graph=True)[0]\n",
    "        hessian = torch.tensor([], device = Proj_device)\n",
    "        for j in range(len(grad)):\n",
    "            u_xxi = torch.autograd.grad(grad[j], x_i, grad_outputs=torch.ones_like(grad[j]), retain_graph=True,create_graph=True, allow_unused=True)[0]           \n",
    "            hessian = torch.cat((hessian, u_xxi.unsqueeze(0)))\n",
    "        Hessian = torch.cat((Hessian, hessian.unsqueeze(0)),dim = 0)\n",
    "    return Hessian\n",
    "\n",
    "def pde_residual(model, t, x, H, M, sigma, alpha, C, D):\n",
    "    \n",
    "    input = torch.cat((t.unsqueeze(1), x),dim=1)\n",
    "    \n",
    "    u = model(input)\n",
    "\n",
    "    u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), create_graph=True, retain_graph=True)[0]\n",
    "    u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    u_xx = get_hessian(u_x,x)\n",
    "\n",
    "#    u_xx = get_hessian_(model,t,x)\n",
    "    \n",
    "    residual = u_t + 0.5 * torch.einsum('bii->b', sigma @ sigma.transpose(1,2) @ u_xx) + (u_x.unsqueeze(1) @ (H @ x.unsqueeze(1).transpose(1,2)) + u_x.unsqueeze(1) @ M @ alpha.unsqueeze(1).transpose(1,2) + x.unsqueeze(1) @ C @ x.unsqueeze(1).transpose(1,2) + alpha.unsqueeze(1) @ D @ alpha.unsqueeze(1).transpose(1,2)).squeeze()\n",
    "    \n",
    "    return residual\n",
    "\n",
    "def boundary_condition(model, t, x, R, T):\n",
    "\n",
    "    \n",
    "    T_input = T * torch.ones_like(t)\n",
    "\n",
    "    input = torch.cat((T_input.unsqueeze(1), x),dim=1)\n",
    "    u = model(input)\n",
    "\n",
    "    return u - (x.unsqueeze(1) @ R @ x.unsqueeze(1).transpose(1,2)).squeeze()\n",
    "\n",
    "def total_residual(model, t, x, H, M, sigma, alpha, C, D, R, T):\n",
    "    \n",
    "    residual_loss = pde_residual(model, t, x, H, M, sigma, alpha, C, D).pow(2).mean()\n",
    "    boundary_loss = boundary_condition(model, t, x, R, T).pow(2).mean()\n",
    "    \n",
    "    return residual_loss + boundary_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def new_data(T, num_samples):\n",
    "    # num_samples = 1000\n",
    "    t_samples = T * torch.rand(num_samples, dtype=Proj_dtype, device = Proj_device, requires_grad=False)\n",
    "    x_ends = torch.tensor([-3,3], dtype = Proj_dtype)\n",
    "    x_samples = x_ends[0] + (x_ends[1]- x_ends[0]) * torch.rand(num_samples , 2, dtype=Proj_dtype, device = Proj_device, requires_grad=False)\n",
    "    return t_samples,x_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define matrices for LQR problem\n",
    "\n",
    "H = torch.tensor([[1.2, 0.8], [-0.6, 0.9]], dtype=Proj_dtype, device = Proj_device)\n",
    "M = torch.tensor([[0.5,0.7], [0.3,1.0]], dtype=Proj_dtype, device = Proj_device)\n",
    "sigma = torch.tensor([[[0.08],[0.11]]], dtype=Proj_dtype, device = Proj_device)\n",
    "\n",
    "C = torch.tensor([[1.6, 0.0], [0.0, 1.1]], dtype=Proj_dtype, device = Proj_device)\n",
    "D = torch.tensor([[0.5, 0.0], [0.0, 0.7]], dtype=Proj_dtype, device = Proj_device)\n",
    "R = torch.tensor([[0.9, 0.0], [0.0, 1.0]], dtype=Proj_dtype, device = Proj_device)\n",
    "T = torch.tensor(1.0, dtype=Proj_dtype, device = Proj_device)\n",
    "\n",
    "solver = LQRSolver(H, M, sigma, C, D, R, T=T, method=\"euler\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1: t_data = 0.7591819184958559, x_data = tensor([-2.0261,  2.9040], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t_samples, x_samples = new_data(T, 1000)\n",
    "\n",
    "\n",
    "def get_data_by_index(iter_p, t_samples, x_samples):\n",
    "    \"\"\"\n",
    "    根据索引o获取对应的t_data和x_data。\n",
    "    \n",
    "    参数:\n",
    "    - o: 索引，从1开始计数。\n",
    "    - t_samples: 包含t数据的张量。\n",
    "    - x_samples: 包含x数据的张量。\n",
    "    \n",
    "    返回:\n",
    "    - t_data: 对应索引的t数据。\n",
    "    - x_data: 对应索引的x数据。\n",
    "    \"\"\"\n",
    "    # 确保o从1开始，因此需要减1来匹配Python的索引从0开始\n",
    "    index = iter_p - 1\n",
    "    # 获取对应索引的数据\n",
    "    t_data = t_samples[index]\n",
    "    x_data = x_samples[index]\n",
    "    return t_data, x_data\n",
    "\n",
    "# 使用示例\n",
    "o = 1  # 假设你想获取第一个样本的数据\n",
    "t_data, x_data = get_data_by_index(o, t_samples, x_samples)\n",
    "print(f\"Sample {o}: t_data = {t_data}, x_data = {x_data}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_t,x_t = new_data(T,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2856, -2.5480],\n",
       "        [-1.5361, -2.1179],\n",
       "        [ 2.9114, -2.4562],\n",
       "        [-0.8773, -0.3769],\n",
       "        [ 0.0379, -0.8511]], dtype=torch.float64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_DGM_test = ControlNN_YYBver().double()\n",
    "input = torch.cat((t_t.unsqueeze(1), x_t),dim=1)\n",
    "\n",
    "a_t = control_DGM_test(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0640,  0.7670],\n",
       "        [ 0.3452,  0.8818],\n",
       "        [ 0.2939, -0.9116],\n",
       "        [-0.8331, -0.0436],\n",
       "        [ 0.0493, -0.0433]], dtype=torch.float64, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = alpha.transpose(1,2).squeeze(0)\n",
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4138]],\n",
       "\n",
       "        [[0.6039]],\n",
       "\n",
       "        [[0.6250]],\n",
       "\n",
       "        [[0.3484]],\n",
       "\n",
       "        [[0.0025]]], dtype=torch.float64, grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_t.unsqueeze(1) @ D @ a_t.unsqueeze(1).transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.2000]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha.unsqueeze(1) @ D @ alpha.unsqueeze(1).transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 9.7858]],\n",
       "\n",
       "        [[ 8.7090]],\n",
       "\n",
       "        [[20.1982]],\n",
       "\n",
       "        [[ 1.3879]],\n",
       "\n",
       "        [[ 0.7991]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_t.unsqueeze(1) @ C @ x_t.unsqueeze(1).transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[10.9858]],\n",
       "\n",
       "        [[ 9.9090]],\n",
       "\n",
       "        [[21.3982]],\n",
       "\n",
       "        [[ 2.5879]],\n",
       "\n",
       "        [[ 1.9991]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_t.unsqueeze(1) @ C @ x_t.unsqueeze(1).transpose(1,2) + alpha.transpose(1,2) @ D @ alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.2000]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha.transpose(1,2) @ D @ alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update of value function\n",
    "\n",
    "def value_update(H, M, sigma, alpha, C, D, R, T): \n",
    "\n",
    "    model_DGM = DGMNN_YYBver().double()\n",
    "    optimizer_DGM = torch.optim.Adam(model_DGM.parameters(), lr=0.0001)\n",
    "    scheduler_DGM = lr_scheduler.ExponentialLR(optimizer_DGM, gamma=0.9)\n",
    "\n",
    "\n",
    "    continue_training = input(\"Do you want to continue training or start a new one? (c/n): \").lower() == 'c'\n",
    "\n",
    "    model_save_path = 'Exercise4/model_DGM_state_dict.pt'\n",
    "    optimizer_save_path = 'Exercise4/optimizer_DGM_state.pt'\n",
    "\n",
    "    if continue_training and os.path.exists(model_save_path) and os.path.exists(optimizer_save_path):\n",
    "        \n",
    "        model_DGM.load_state_dict(torch.load(model_save_path))\n",
    "        optimizer_DGM.load_state_dict(torch.load(optimizer_save_path))\n",
    "        print(\"Continuing training from saved state.\")\n",
    "    else:\n",
    "        print(\"Starting training from scratch.\")\n",
    "\n",
    "    epoch_losses = []\n",
    "\n",
    "    iterations = 50\n",
    "    epochs = 100\n",
    "\n",
    "    patience = 10\n",
    "\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "    filename = f'Exercise4/value_training_loss_{timestamp}.dat'\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "\n",
    "        for iteration in range(iterations):\n",
    "            print(f'Iteration {iteration+1}/{iterations}'+'\\n')\n",
    "            \n",
    "            dataset = TensorDataset(t_data,x_data)\n",
    "            dataloader = DataLoader(dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "            best_loss = float('inf')\n",
    "            patience_counter = 0  \n",
    "\n",
    "            for epoch in range(epochs):\n",
    "\n",
    "                model_DGM.train()\n",
    "                total_loss = 0\n",
    "                \n",
    "                for batch_idx, (_t_data,_x_data) in enumerate(dataloader):\n",
    "                    optimizer_DGM.zero_grad()\n",
    "                    t_data = _t_data.clone().requires_grad_(True)\n",
    "                    x_data = _x_data.clone().requires_grad_(True)\n",
    "                    loss = total_residual(model_DGM, t_data, x_data, H, M, sigma, alpha, C, D, R, T) \n",
    "                    loss.backward()\n",
    "                    optimizer_DGM.step()\n",
    "                    total_loss += loss.item()\n",
    "                \n",
    "                avg_loss = total_loss / len(dataloader)\n",
    "                epoch_losses.append(avg_loss)\n",
    "                \n",
    "                scheduler_DGM.step()\n",
    "\n",
    "                f.write(f'Iteration {iteration+1}, Epoch {epoch+1}, Loss: {avg_loss}\\n')\n",
    "\n",
    "                if avg_loss < best_loss:\n",
    "                    best_loss = avg_loss\n",
    "                    patience_counter = 0  \n",
    "                    torch.save(model_DGM.state_dict(), model_save_path)\n",
    "                    torch.save(optimizer_DGM.state_dict(), optimizer_save_path)\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "\n",
    "                if epoch == 0 or (epoch+1) % 5 == 0:\n",
    "                    print(f'Epoch {epoch+1}/{epochs} \\t Loss: {avg_loss}')\n",
    "                \n",
    "                \n",
    "                if patience_counter >= patience:\n",
    "                    print(f'Early stopping triggered at epoch {epoch+1}')\n",
    "                    break  \n",
    "            print('\\n')\n",
    "    return  model_DGM(torch.cat((t_data.unsqueeze(1), x_data),dim=1)).squeeze()     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hamiltonian\n",
    "\n",
    "def Hamiltonian(alpha, current_value, x, H, M, C, D):\n",
    "\n",
    "\n",
    "    u = current_value\n",
    "    a = alpha\n",
    "\n",
    "\n",
    "    u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    H_i  = u_x.unsqueeze(1) @ H @ x.unsqueeze(1).transpose(1,2) + u_x.unsqueeze(1) @ M @ a.unsqueeze(1).transpose(1,2) + x.unsqueeze(1) @ C @ x.unsqueeze(1).transpose(1,2) + a.unsqueeze(1) @ D @ a.unsqueeze(1).transpose(1,2)\n",
    "    \n",
    "    #  The true hamiltonian\n",
    "    H  = H_i.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update of control\n",
    "\n",
    "def control_update(alpha, current_value, x_data, H, M, C, D): \n",
    "\n",
    "    control_DGM = ControlNN_YYBver().double()\n",
    "    optimizer_control = torch.optim.Adam(control_DGM.parameters(), lr=0.0001)\n",
    "    scheduler_control = lr_scheduler.ExponentialLR(optimizer_control, gamma=0.9)\n",
    "\n",
    "\n",
    "    continue_training = input(\"Do you want to continue training or start a new one? (c/n): \").lower() == 'c'\n",
    "\n",
    "    model_save_path = 'Exercise4/control_DGM_state_dict.pt'\n",
    "    optimizer_save_path = 'Exercise4/optimizer_control_state.pt'\n",
    "\n",
    "    if continue_training and os.path.exists(model_save_path) and os.path.exists(optimizer_save_path):\n",
    "        \n",
    "        control_DGM.load_state_dict(torch.load(model_save_path))\n",
    "        optimizer_control.load_state_dict(torch.load(optimizer_save_path))\n",
    "        print(\"Continuing training from saved state.\")\n",
    "    else:\n",
    "        print(\"Starting training from scratch.\")\n",
    "\n",
    "    epoch_losses = []\n",
    "\n",
    "    iterations = 50\n",
    "    epochs = 100\n",
    "\n",
    "    patience = 10\n",
    "\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "    filename = f'Exercise4/control_training_loss_{timestamp}.dat'\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "\n",
    "        for iteration in range(iterations):\n",
    "            print(f'Iteration {iteration+1}/{iterations}'+'\\n')\n",
    "            \n",
    "\n",
    "            dataset = TensorDataset(t_data,x_data)\n",
    "            dataloader = DataLoader(dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "            best_loss = float('inf')\n",
    "            patience_counter = 0  \n",
    "\n",
    "            for epoch in range(epochs):\n",
    "\n",
    "                control_DGM.train()\n",
    "                total_loss = 0\n",
    "                \n",
    "                for batch_idx, (_t_data,_x_data) in enumerate(dataloader):\n",
    "                    optimizer_control.zero_grad()\n",
    "                    t_data = _t_data.clone().requires_grad_(True)\n",
    "                    x_data = _x_data.clone().requires_grad_(True)\n",
    "                    loss = Hamiltonian(alpha, current_value, x_data, H, M, C, D) \n",
    "                    loss.backward()\n",
    "                    optimizer_control.step()\n",
    "                    total_loss += loss.item()\n",
    "                \n",
    "                avg_loss = total_loss / len(dataloader)\n",
    "                epoch_losses.append(avg_loss)\n",
    "                \n",
    "                scheduler_control.step()\n",
    "\n",
    "                f.write(f'Iteration {iteration+1}, Epoch {epoch+1}, Loss: {avg_loss}\\n')\n",
    "\n",
    "                if avg_loss < best_loss:\n",
    "                    best_loss = avg_loss\n",
    "                    patience_counter = 0  \n",
    "                    torch.save(control_DGM.state_dict(), model_save_path)\n",
    "                    torch.save(optimizer_control.state_dict(), optimizer_save_path)\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "\n",
    "                if epoch == 0 or (epoch+1) % 5 == 0:\n",
    "                    print(f'Epoch {epoch+1}/{epochs} \\t Loss: {avg_loss}')\n",
    "                \n",
    "                \n",
    "                if patience_counter >= patience:\n",
    "                    print(f'Early stopping triggered at epoch {epoch+1}')\n",
    "                    break  \n",
    "            print('\\n')\n",
    "        \n",
    "    return control_DGM(torch.cat((t_data.unsqueeze(1), x_data),dim=1)).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from scratch.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Exercise4/value_training_loss_2024-04-10_18-22-47.dat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/lu/Desktop/SCDAA/Exercise4_1 copy.ipynb Cell 22\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lu/Desktop/SCDAA/Exercise4_1%20copy.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m alpha \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([[[\u001b[39m1\u001b[39m],[\u001b[39m1\u001b[39m]]], dtype\u001b[39m=\u001b[39mProj_dtype, device \u001b[39m=\u001b[39m Proj_device)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lu/Desktop/SCDAA/Exercise4_1%20copy.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m iter_p \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m999\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lu/Desktop/SCDAA/Exercise4_1%20copy.ipynb#X32sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     current_value \u001b[39m=\u001b[39m value_update(H, M, sigma, alpha, C, D, R, T)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lu/Desktop/SCDAA/Exercise4_1%20copy.ipynb#X32sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     alpha \u001b[39m=\u001b[39m control_update(alpha, current_value, x_data, H, M, C, D)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lu/Desktop/SCDAA/Exercise4_1%20copy.ipynb#X32sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     t_data \u001b[39m=\u001b[39m t_samples[iter_p]\n",
      "\u001b[1;32m/Users/lu/Desktop/SCDAA/Exercise4_1 copy.ipynb Cell 22\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lu/Desktop/SCDAA/Exercise4_1%20copy.ipynb#X32sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m timestamp \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow()\u001b[39m.\u001b[39mstrftime(\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm-\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m%\u001b[39m\u001b[39mH-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lu/Desktop/SCDAA/Exercise4_1%20copy.ipynb#X32sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m filename \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mExercise4/value_training_loss_\u001b[39m\u001b[39m{\u001b[39;00mtimestamp\u001b[39m}\u001b[39;00m\u001b[39m.dat\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lu/Desktop/SCDAA/Exercise4_1%20copy.ipynb#X32sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(filename, \u001b[39m'\u001b[39;49m\u001b[39mw\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lu/Desktop/SCDAA/Exercise4_1%20copy.ipynb#X32sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     \u001b[39mfor\u001b[39;00m iteration \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(iterations):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lu/Desktop/SCDAA/Exercise4_1%20copy.ipynb#X32sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mIteration \u001b[39m\u001b[39m{\u001b[39;00miteration\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00miterations\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Exercise4/value_training_loss_2024-04-10_18-22-47.dat'"
     ]
    }
   ],
   "source": [
    "# Policy Iteration\n",
    "\n",
    "alpha = torch.tensor([[[1],[1]]], dtype=Proj_dtype, device = Proj_device)\n",
    "for iter_p in range(999):\n",
    "    current_value = value_update(H, M, sigma, alpha, C, D, R, T)\n",
    "    alpha = control_update(alpha, current_value, x_data, H, M, C, D)\n",
    "    t_data = t_samples[iter_p]\n",
    "    x_data = x_samples[iter_p]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
