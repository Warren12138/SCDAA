{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from lib.Exercise1_1 import LQRSolver\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "class DGMNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DGMNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(3, 100)  \n",
    "        self.layer2 = nn.Linear(100, 100)\n",
    "        self.layer3 = nn.Linear(100, 100)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.output = nn.Linear(100, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.tanh(self.layer1(x))\n",
    "        x = self.tanh(self.layer2(x))\n",
    "        x = self.tanh(self.layer3(x))\n",
    "        return self.output(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hessian(grad,x):\n",
    "    Hessian = torch.tensor([])\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        hessian = torch.tensor([])\n",
    "        for j in range(len(grad[i])):\n",
    "            u_xxi = torch.autograd.grad(grad[i][j], x, grad_outputs=torch.ones_like(grad[i][j]), retain_graph=True,create_graph=True, allow_unused=True)[0]           \n",
    "            hessian = torch.cat((hessian, u_xxi[i].unsqueeze(0)))\n",
    "        Hessian = torch.cat((Hessian, hessian.unsqueeze(0)),dim = 0)\n",
    "    return Hessian\n",
    "\n",
    "def pde_residual(model, t, x):\n",
    "    \n",
    "    input = torch.cat((t.unsqueeze(1), x),dim=1)\n",
    "    \n",
    "    u = model(input)\n",
    "\n",
    "    u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), create_graph=True, retain_graph=True)[0]\n",
    "    u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    u_xx = get_hessian(u_x,x)\n",
    "\n",
    "    residual = u_t + 0.5 * torch.einsum('bii->b', sigma @ sigma.transpose(1,2) @ u_xx) + (u_x.unsqueeze(1) @ (H @ x.unsqueeze(1).transpose(1,2)) + u_x.unsqueeze(1) @ M @ alpha + x.unsqueeze(1) @ C @ x.unsqueeze(1).transpose(1,2) + alpha.transpose(1,2) @ D @ alpha).squeeze()\n",
    "    return residual\n",
    "\n",
    "def boundary_condition(model,t, x):\n",
    "    # 定义边界条件的残差\n",
    "    \n",
    "    T_input = T * torch.ones_like(t)\n",
    "\n",
    "    input = torch.cat((T_input.unsqueeze(1), x),dim=1)\n",
    "    u = model(input)\n",
    "\n",
    "    return u - (x.unsqueeze(1) @ R @ x.unsqueeze(1).transpose(1,2)).squeeze()\n",
    "\n",
    "def total_residual(model, t, x):\n",
    "    # 计算内部PDE残差\n",
    "    residual_loss = pde_residual(model, t, x).pow(2).mean()\n",
    "    # 计算边界条件的残差\n",
    "    boundary_loss = boundary_condition(model,t,x).pow(2).mean()\n",
    "    \n",
    "    return residual_loss + boundary_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_data(num_samples):\n",
    "    #num_samples = 10000\n",
    "    t_samples = T * torch.rand(num_samples, dtype=torch.double, requires_grad=True)\n",
    "    x_ends = torch.tensor([-3,3], dtype = torch.double)\n",
    "    x_samples = x_ends[0] + (x_ends[1]- x_ends[0]) * torch.rand(num_samples , 2, dtype=torch.double, requires_grad=True)\n",
    "    return t_samples,x_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define matrices for LQR problem\n",
    "H = torch.tensor([[1.2, 0.8], [-0.6, 0.9]], dtype=torch.double)\n",
    "M = torch.tensor([[0.5,0.7], [0.3,1.0]], dtype=torch.double)\n",
    "sigma = torch.tensor([[[0.8],[1.1]]], dtype=torch.double)\n",
    "alpha = torch.tensor([[[1],[1]]], dtype=torch.double)\n",
    "C = torch.tensor([[1.6, 0.0], [0.0, 1.1]], dtype=torch.double)\n",
    "D = torch.tensor([[0.5, 0.0], [0.0, 0.7]], dtype=torch.double)\n",
    "R = torch.tensor([[0.9, 0.0], [0.0, 1.0]], dtype=torch.double)\n",
    "T = torch.tensor(1.0, dtype=torch.double)\n",
    "\n",
    "solver = LQRSolver(H, M, sigma, C, D, R, T=T, method=\"euler\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 179.38920943757603\n",
      "Epoch 100/500 \t Loss: 55.63965676485591\n",
      "Epoch 200/500 \t Loss: 21.000615344534598\n",
      "Epoch 300/500 \t Loss: 16.933412291149832\n",
      "Epoch 400/500 \t Loss: 15.0249363441466\n",
      "Epoch 500/500 \t Loss: 13.844563649177934\n",
      "Batch 2/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 19.503522725018385\n",
      "Epoch 100/500 \t Loss: 15.59205568977103\n",
      "Epoch 200/500 \t Loss: 14.961941164535304\n",
      "Epoch 300/500 \t Loss: 14.578325733578623\n",
      "Epoch 400/500 \t Loss: 14.280125460164486\n",
      "Epoch 500/500 \t Loss: 14.020513356986141\n",
      "Batch 3/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 25.340556877811157\n",
      "Epoch 100/500 \t Loss: 19.11800524720242\n",
      "Epoch 200/500 \t Loss: 18.613381802375137\n",
      "Epoch 300/500 \t Loss: 18.314198112088807\n",
      "Epoch 400/500 \t Loss: 18.106350705380343\n",
      "Epoch 500/500 \t Loss: 17.96623103348157\n",
      "Batch 4/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 18.316024904658608\n",
      "Epoch 100/500 \t Loss: 14.321167523864505\n",
      "Epoch 200/500 \t Loss: 14.088331209657527\n",
      "Epoch 300/500 \t Loss: 13.95257659499725\n",
      "Epoch 400/500 \t Loss: 13.880596541372851\n",
      "Epoch 500/500 \t Loss: 13.838540642104649\n",
      "Batch 5/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 16.603201980423833\n",
      "Epoch 100/500 \t Loss: 10.893308765419468\n",
      "Epoch 200/500 \t Loss: 10.799006848406007\n",
      "Epoch 300/500 \t Loss: 10.76169036124822\n",
      "Epoch 400/500 \t Loss: 10.740574049237525\n",
      "Epoch 500/500 \t Loss: 10.725385042820179\n",
      "Batch 6/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 21.0254261161987\n",
      "Epoch 100/500 \t Loss: 12.77113418758258\n",
      "Epoch 200/500 \t Loss: 12.691954921918315\n",
      "Epoch 300/500 \t Loss: 12.654403327316457\n",
      "Epoch 400/500 \t Loss: 12.633358100252506\n",
      "Epoch 500/500 \t Loss: 12.620517919690661\n",
      "Batch 7/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 19.22261260790122\n",
      "Epoch 100/500 \t Loss: 11.854642696797141\n",
      "Epoch 200/500 \t Loss: 11.806748574359398\n",
      "Epoch 300/500 \t Loss: 11.787555029846107\n",
      "Epoch 400/500 \t Loss: 11.776965983310944\n",
      "Epoch 500/500 \t Loss: 11.770306922797884\n",
      "Batch 8/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 17.70131773985912\n",
      "Epoch 100/500 \t Loss: 14.267154729980172\n",
      "Epoch 200/500 \t Loss: 14.204802726323136\n",
      "Epoch 300/500 \t Loss: 14.189355026344703\n",
      "Epoch 400/500 \t Loss: 14.18399222345668\n",
      "Epoch 500/500 \t Loss: 14.18108874727821\n",
      "Batch 9/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 14.405415098152291\n",
      "Epoch 100/500 \t Loss: 12.15503977986431\n",
      "Epoch 200/500 \t Loss: 12.13593010982086\n",
      "Epoch 300/500 \t Loss: 12.13070571261003\n",
      "Epoch 400/500 \t Loss: 12.12851968289012\n",
      "Epoch 500/500 \t Loss: 12.127126819438834\n",
      "Batch 10/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 17.076769226688942\n",
      "Epoch 100/500 \t Loss: 12.650131537774369\n",
      "Epoch 200/500 \t Loss: 12.621707882321694\n",
      "Epoch 300/500 \t Loss: 12.61170053249242\n",
      "Epoch 400/500 \t Loss: 12.607536671351454\n",
      "Epoch 500/500 \t Loss: 12.605548523226597\n",
      "Batch 11/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 17.879089137743428\n",
      "Epoch 100/500 \t Loss: 11.388772659728536\n",
      "Epoch 200/500 \t Loss: 11.346630834342934\n",
      "Epoch 300/500 \t Loss: 11.332133713612906\n",
      "Epoch 400/500 \t Loss: 11.325073311817595\n",
      "Epoch 500/500 \t Loss: 11.320870767780143\n",
      "Batch 12/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 18.192793433834126\n",
      "Epoch 100/500 \t Loss: 15.824185397319265\n",
      "Epoch 200/500 \t Loss: 15.787821839306956\n",
      "Epoch 300/500 \t Loss: 15.778305067565379\n",
      "Epoch 400/500 \t Loss: 15.774780168344245\n",
      "Epoch 500/500 \t Loss: 15.7729576774973\n",
      "Batch 13/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 15.12153911170846\n",
      "Epoch 100/500 \t Loss: 12.969719917332302\n",
      "Epoch 200/500 \t Loss: 12.954158350955115\n",
      "Epoch 300/500 \t Loss: 12.947761605282874\n",
      "Epoch 400/500 \t Loss: 12.944559217183782\n",
      "Epoch 500/500 \t Loss: 12.9425420557071\n",
      "Batch 14/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 17.428828288865482\n",
      "Epoch 100/500 \t Loss: 15.004155484798455\n",
      "Epoch 200/500 \t Loss: 14.993899121782515\n",
      "Epoch 300/500 \t Loss: 14.989638230589938\n",
      "Epoch 400/500 \t Loss: 14.987351396159916\n",
      "Epoch 500/500 \t Loss: 14.985935458104187\n",
      "Batch 15/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 21.67395585378349\n",
      "Epoch 100/500 \t Loss: 19.130457348665402\n",
      "Epoch 200/500 \t Loss: 19.111021064626097\n",
      "Epoch 300/500 \t Loss: 19.10463702228961\n",
      "Epoch 400/500 \t Loss: 19.101376893511357\n",
      "Epoch 500/500 \t Loss: 19.099403028586366\n",
      "Batch 16/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 15.839748087205068\n",
      "Epoch 100/500 \t Loss: 13.81575445748356\n",
      "Epoch 200/500 \t Loss: 13.804348901987217\n",
      "Epoch 300/500 \t Loss: 13.801507605187632\n",
      "Epoch 400/500 \t Loss: 13.800227820706219\n",
      "Epoch 500/500 \t Loss: 13.799421370277082\n",
      "Batch 17/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 14.330985374106621\n",
      "Epoch 100/500 \t Loss: 10.938275950109585\n",
      "Epoch 200/500 \t Loss: 10.931499582105687\n",
      "Epoch 300/500 \t Loss: 10.9289099191339\n",
      "Epoch 400/500 \t Loss: 10.927363219733197\n",
      "Epoch 500/500 \t Loss: 10.926270126546994\n",
      "Batch 18/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 17.217842260171746\n",
      "Epoch 100/500 \t Loss: 15.838128275821221\n",
      "Epoch 200/500 \t Loss: 15.823587394772606\n",
      "Epoch 300/500 \t Loss: 15.818064006232476\n",
      "Epoch 400/500 \t Loss: 15.814796954362\n",
      "Epoch 500/500 \t Loss: 15.812622448193757\n",
      "Batch 19/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 15.818213782627524\n",
      "Epoch 100/500 \t Loss: 13.42515321617837\n",
      "Epoch 200/500 \t Loss: 13.415542270759731\n",
      "Epoch 300/500 \t Loss: 13.41243184755704\n",
      "Epoch 400/500 \t Loss: 13.411105000598141\n",
      "Epoch 500/500 \t Loss: 13.410457584962652\n",
      "Batch 20/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 11.230250088660055\n",
      "Epoch 100/500 \t Loss: 9.411603714577025\n",
      "Epoch 200/500 \t Loss: 9.400992662962922\n",
      "Epoch 300/500 \t Loss: 9.397263971337486\n",
      "Epoch 400/500 \t Loss: 9.395333204864055\n",
      "Epoch 500/500 \t Loss: 9.39413285813998\n",
      "Batch 21/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 11.923259699628279\n",
      "Epoch 100/500 \t Loss: 10.079914843287808\n",
      "Epoch 200/500 \t Loss: 10.068834999968193\n",
      "Epoch 300/500 \t Loss: 10.065174731579814\n",
      "Epoch 400/500 \t Loss: 10.063609715677979\n",
      "Epoch 500/500 \t Loss: 10.062831174918266\n",
      "Batch 22/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 13.524170602346674\n",
      "Epoch 100/500 \t Loss: 12.44271573204784\n",
      "Epoch 200/500 \t Loss: 12.437439794988011\n",
      "Epoch 300/500 \t Loss: 12.434824979040881\n",
      "Epoch 400/500 \t Loss: 12.433322883921997\n",
      "Epoch 500/500 \t Loss: 12.432401185857836\n",
      "Batch 23/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 10.921914816427755\n",
      "Epoch 100/500 \t Loss: 10.152609573353306\n",
      "Epoch 200/500 \t Loss: 10.148826486471584\n",
      "Epoch 300/500 \t Loss: 10.147547726876777\n",
      "Epoch 400/500 \t Loss: 10.14696523453958\n",
      "Epoch 500/500 \t Loss: 10.146632403694523\n",
      "Batch 24/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 20.24021652350002\n",
      "Epoch 100/500 \t Loss: 18.055442250345322\n",
      "Epoch 200/500 \t Loss: 18.049309440131598\n",
      "Epoch 300/500 \t Loss: 18.04610508760644\n",
      "Epoch 400/500 \t Loss: 18.04403459533716\n",
      "Epoch 500/500 \t Loss: 18.042642271151472\n",
      "Batch 25/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 16.818568380085843\n",
      "Epoch 100/500 \t Loss: 15.029096630993733\n",
      "Epoch 200/500 \t Loss: 15.022937190112058\n",
      "Epoch 300/500 \t Loss: 15.020379733945404\n",
      "Epoch 400/500 \t Loss: 15.019018655927374\n",
      "Epoch 500/500 \t Loss: 15.018195033539751\n",
      "Batch 26/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 13.904689424309534\n",
      "Epoch 100/500 \t Loss: 10.95374325142318\n",
      "Epoch 200/500 \t Loss: 10.947914167650236\n",
      "Epoch 300/500 \t Loss: 10.946182000318519\n",
      "Epoch 400/500 \t Loss: 10.945504307201627\n",
      "Epoch 500/500 \t Loss: 10.945171571373075\n",
      "Batch 27/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 17.126471635678392\n",
      "Epoch 100/500 \t Loss: 15.520214585780147\n",
      "Epoch 200/500 \t Loss: 15.509972885680435\n",
      "Epoch 300/500 \t Loss: 15.505880205856196\n",
      "Epoch 400/500 \t Loss: 15.50395921895383\n",
      "Epoch 500/500 \t Loss: 15.502969857620636\n",
      "Batch 28/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 14.824829943732531\n",
      "Epoch 100/500 \t Loss: 13.649976110050158\n",
      "Epoch 200/500 \t Loss: 13.64611006148198\n",
      "Epoch 300/500 \t Loss: 13.645038495106963\n",
      "Epoch 400/500 \t Loss: 13.644617444516044\n",
      "Epoch 500/500 \t Loss: 13.644399030959615\n",
      "Batch 29/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 12.001834386220537\n",
      "Epoch 100/500 \t Loss: 9.98238320194046\n",
      "Epoch 200/500 \t Loss: 9.976862938181611\n",
      "Epoch 300/500 \t Loss: 9.974973102175106\n",
      "Epoch 400/500 \t Loss: 9.974069938138461\n",
      "Epoch 500/500 \t Loss: 9.973547992460126\n",
      "Batch 30/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 13.946458958981626\n",
      "Epoch 100/500 \t Loss: 12.215250434477516\n",
      "Epoch 200/500 \t Loss: 12.212002043167447\n",
      "Epoch 300/500 \t Loss: 12.210672167365013\n",
      "Epoch 400/500 \t Loss: 12.209912113896316\n",
      "Epoch 500/500 \t Loss: 12.209413820040151\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DGMNN(\n",
       "  (layer1): Linear(in_features=3, out_features=50, bias=True)\n",
       "  (layer2): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (tanh): Tanh()\n",
       "  (output): Linear(in_features=50, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_DGM = DGMNN().double()\n",
    "\n",
    "# Prepare for training\n",
    "optimizer_DGM = torch.optim.Adam(model_DGM.parameters(), lr=0.001)\n",
    "epoch_losses = []\n",
    "\n",
    "batch_size = 30\n",
    "epochs = 500\n",
    "\n",
    "for batch in range(batch_size):\n",
    "    print(f'Batch {batch+1}/{batch_size}'+'\\n')\n",
    "    \n",
    "    t_data,x_data = new_data(50)\n",
    "    dataset = TensorDataset(t_data,x_data)\n",
    "    dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        model_DGM.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_idx, (t_data_,x_data_) in enumerate(dataloader):\n",
    "            optimizer_DGM.zero_grad()\n",
    "            loss = total_residual(model_DGM, t_data_, x_data_) \n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer_DGM.step()\n",
    "            total_loss += loss.item()\n",
    "        epoch_losses.append(total_loss / len(dataloader))\n",
    "\n",
    "        if epoch == 0:\n",
    "            print(f'Epoch {epoch+1}/{epochs} \\t Loss: {total_loss / len(dataloader)}')\n",
    "        if (epoch+1) % 100 == 0:\n",
    "            print(f'Epoch {epoch+1}/{epochs} \\t Loss: {total_loss / len(dataloader)}')\n",
    "\n",
    "print('\\n')\n",
    "model_DGM.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "t_data,x_data = new_data(5)\n",
    "\n",
    "value_numerical = solver.value_function(t_data.detach(),x_data.detach().unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = model_DGM(torch.cat((t_data.unsqueeze(1), x_data.squeeze(1)),dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([13.8489,  7.5302,  8.5210,  7.5368, 14.1952], dtype=torch.float64,\n",
       "       grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([39.2751, 27.3157,  9.1775, 18.6481, 10.6329], dtype=torch.float64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_numerical"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
