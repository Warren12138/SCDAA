{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "from lib.Exercise1_1 import LQRSolver\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "Proj_dtype = torch.double\n",
    "Proj_device = 'cpu' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGMhiddenlayerYYBver(nn.Module):\n",
    "\n",
    "    # From the original paper of Justin's, presented by Yuebo Yang Apr.9th 2024\n",
    "    \n",
    "    def __init__(self, input_f, output_f, activation = 'tanh'):\n",
    "        \n",
    "        super(DGMhiddenlayerYYBver, self).__init__()\n",
    "\n",
    "        self.input_f = input_f\n",
    "        self.output_f = output_f\n",
    "\n",
    "        # Params\n",
    "\n",
    "        # Zl's\n",
    "\n",
    "        self.Uzl = nn.Parameter(torch.Tensor(output_f, input_f))\n",
    "        self.Wzl = nn.Parameter(torch.Tensor(output_f, output_f))\n",
    "        self.bzl = nn.Parameter(torch.Tensor(output_f))\n",
    "\n",
    "        # Gl's\n",
    "\n",
    "        self.Ugl = nn.Parameter(torch.Tensor(output_f, input_f))\n",
    "        self.Wgl = nn.Parameter(torch.Tensor(output_f, output_f))\n",
    "        self.bgl = nn.Parameter(torch.Tensor(output_f))\n",
    "\n",
    "        # Rl's\n",
    "\n",
    "        self.Url = nn.Parameter(torch.Tensor(output_f, input_f))\n",
    "        self.Wrl = nn.Parameter(torch.Tensor(output_f, output_f))\n",
    "        self.brl = nn.Parameter(torch.Tensor(output_f))\n",
    "\n",
    "        # Hl's\n",
    "\n",
    "        self.Uhl = nn.Parameter(torch.Tensor(output_f, input_f))\n",
    "        self.Whl = nn.Parameter(torch.Tensor(output_f, output_f))\n",
    "        self.bhl = nn.Parameter(torch.Tensor(output_f))\n",
    "\n",
    "\n",
    "        if activation == 'tanh':\n",
    "            self.activation = torch.tanh\n",
    "        else:\n",
    "            self.activation = None \n",
    "\n",
    "        self.init_method = 'normal' # or 'uniform'\n",
    "\n",
    "        self._initialize_params()\n",
    "\n",
    "    def _initialize_params(self):\n",
    "\n",
    "        if self.init_method == 'uniform':\n",
    "            for param in self.parameters():\n",
    "                if param.dim() > 1:\n",
    "                    init.xavier_uniform_(param)  \n",
    "                else:\n",
    "                    init.constant_(param, 0) \n",
    "                    \n",
    "        if self.init_method == 'normal':\n",
    "            for param in self.parameters():\n",
    "                if param.dim() > 1:\n",
    "                    init.xavier_normal_(param)  \n",
    "                else:\n",
    "                    init.constant_(param, 0) \n",
    "\n",
    "    def forward(self, x, S1, Sl):\n",
    "\n",
    "        Zl = self.activation(torch.mm(x, self.Uzl.t())+ torch.mm(Sl, self.Wzl.t()) + self.bzl)\n",
    "\n",
    "        Gl = self.activation(torch.mm(x, self.Ugl.t())+ torch.mm(S1, self.Wgl.t()) + self.bgl)\n",
    "\n",
    "        Rl = self.activation(torch.mm(x, self.Url.t())+ torch.mm(Sl, self.Wrl.t()) + self.brl)\n",
    "\n",
    "        Hl = self.activation(torch.mm(x, self.Uhl.t())+ torch.mm(torch.mul(Sl,Rl), self.Whl.t()) + self.bhl)\n",
    "\n",
    "        Sl_1 = torch.mul((1-Gl),Hl) + torch.mul(Zl,Sl)\n",
    "\n",
    "        return Sl_1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DGMNN_YYBver(nn.Module):\n",
    "\n",
    "    # From the original paper of Justin's, presented by Yuebo Yang Apr.9th 2024\n",
    "\n",
    "    def __init__(self, init_method = 'uniform'):\n",
    "        super(DGMNN_YYBver, self).__init__()\n",
    "\n",
    "        self.nodenum = 50\n",
    "\n",
    "        self.layer1 = DGMhiddenlayerYYBver(3, self.nodenum)\n",
    "        self.layer2 = DGMhiddenlayerYYBver(3, self.nodenum)\n",
    "        self.layer3 = DGMhiddenlayerYYBver(3, self.nodenum)\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # Params\n",
    "\n",
    "        # S1's\n",
    "\n",
    "        self.W1 = nn.Parameter(torch.Tensor(self.nodenum, 3))\n",
    "        self.b1 = nn.Parameter(torch.Tensor(self.nodenum))\n",
    "\n",
    "        # Output's\n",
    "\n",
    "        self.W = nn.Parameter(torch.Tensor(1, self.nodenum))\n",
    "        self.b = nn.Parameter(torch.Tensor(1))\n",
    "\n",
    "        self.activation = torch.tanh\n",
    "\n",
    "        self.init_method = 'normal' # or 'uniform'\n",
    "\n",
    "        self._initialize_params()\n",
    "\n",
    "    def _initialize_params(self):\n",
    "\n",
    "        if self.init_method == 'uniform':\n",
    "            for param in self.parameters():\n",
    "                if param.dim() > 1:\n",
    "                    init.xavier_uniform_(param)  \n",
    "                else:\n",
    "                    init.constant_(param, 0) \n",
    "                    \n",
    "        if self.init_method == 'normal':\n",
    "            for param in self.parameters():\n",
    "                if param.dim() > 1:\n",
    "                    init.xavier_normal_(param)  \n",
    "                else:\n",
    "                    init.constant_(param, 0) \n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        S_1 = self.activation(torch.mm(x, self.W1.t()) + self.b1)\n",
    "        # l=1\n",
    "        S_2 = self.layer1(x,S_1,S_1)\n",
    "        # l=2\n",
    "        S_3 = self.layer2(x,S_1,S_2)\n",
    "        # l=3\n",
    "        S_4 = self.layer3(x,S_1,S_3)\n",
    "\n",
    "        output = torch.mm(S_4, self.W.t()) + self.b\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ControlNN_YYBver(nn.Module):\n",
    "\n",
    "    # From the original paper of Justin's, presented by Yuebo Yang Apr.9th 2024\n",
    "\n",
    "    def __init__(self, init_method = 'uniform'):\n",
    "        super(ControlNN_YYBver, self).__init__()\n",
    "\n",
    "        self.nodenum = 50\n",
    "\n",
    "        self.layer1 = DGMhiddenlayerYYBver(3, self.nodenum)\n",
    "        self.layer2 = DGMhiddenlayerYYBver(3, self.nodenum)\n",
    "        self.layer3 = DGMhiddenlayerYYBver(3, self.nodenum)\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # Params\n",
    "\n",
    "        # S1's\n",
    "\n",
    "        self.W1 = nn.Parameter(torch.Tensor(self.nodenum, 3))\n",
    "        self.b1 = nn.Parameter(torch.Tensor(self.nodenum))\n",
    "\n",
    "        # Output's\n",
    "\n",
    "        self.W = nn.Parameter(torch.Tensor(2, self.nodenum))\n",
    "        self.b = nn.Parameter(torch.Tensor(2))\n",
    "\n",
    "        self.activation = torch.tanh\n",
    "\n",
    "        self.init_method = 'normal' # or 'uniform'\n",
    "\n",
    "        self._initialize_params()\n",
    "\n",
    "    def _initialize_params(self):\n",
    "\n",
    "        if self.init_method == 'uniform':\n",
    "            for param in self.parameters():\n",
    "                if param.dim() > 1:\n",
    "                    init.xavier_uniform_(param)  \n",
    "                else:\n",
    "                    init.constant_(param, 0) \n",
    "                    \n",
    "        if self.init_method == 'normal':\n",
    "            for param in self.parameters():\n",
    "                if param.dim() > 1:\n",
    "                    init.xavier_normal_(param)  \n",
    "                else:\n",
    "                    init.constant_(param, 0) \n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        S_1 = self.activation(torch.mm(x, self.W1.t()) + self.b1)\n",
    "        # l=1\n",
    "        S_2 = self.layer1(x,S_1,S_1)\n",
    "        # l=2\n",
    "        S_3 = self.layer2(x,S_1,S_2)\n",
    "        # l=3\n",
    "        S_4 = self.layer3(x,S_1,S_3)\n",
    "\n",
    "        output = torch.mm(S_4, self.W.t()) + self.b\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hessian(grad,x):\n",
    "    Hessian = torch.tensor([], device = Proj_device)\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        hessian = torch.tensor([], device = Proj_device)\n",
    "        for j in range(len(grad[i])):\n",
    "            u_xxi = torch.autograd.grad(grad[i][j], x, grad_outputs=torch.ones_like(grad[i][j]), retain_graph=True,create_graph=True, allow_unused=True)[0]           \n",
    "            hessian = torch.cat((hessian, u_xxi[i].unsqueeze(0)))\n",
    "        Hessian = torch.cat((Hessian, hessian.unsqueeze(0)),dim = 0)\n",
    "        # print(Hessian)\n",
    "    return Hessian\n",
    "\n",
    "def get_hessian_(model,t,x):\n",
    "    Hessian = torch.tensor([], device = Proj_device)\n",
    "    for i in range(len(t)):\n",
    "        x_i = V(x[i],requires_grad=True)\n",
    "        input = torch.cat(((t[i]).unsqueeze(0), x_i),dim=0)\n",
    "        u_in = model(input)\n",
    "        grad = torch.autograd.grad(u_in, x_i, grad_outputs=torch.ones_like(u_in), create_graph=True, retain_graph=True)[0]\n",
    "        hessian = torch.tensor([], device = Proj_device)\n",
    "        for j in range(len(grad)):\n",
    "            u_xxi = torch.autograd.grad(grad[j], x_i, grad_outputs=torch.ones_like(grad[j]), retain_graph=True,create_graph=True, allow_unused=True)[0]           \n",
    "            hessian = torch.cat((hessian, u_xxi.unsqueeze(0)))\n",
    "        Hessian = torch.cat((Hessian, hessian.unsqueeze(0)),dim = 0)\n",
    "    return Hessian\n",
    "\n",
    "def pde_residual(model, t, x, H, M, sigma, alpha, C, D):\n",
    "    \n",
    "    input = torch.cat((t.unsqueeze(1), x),dim=1)\n",
    "    \n",
    "    u = model(input)\n",
    "\n",
    "    u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), create_graph=True, retain_graph=True)[0]\n",
    "    u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    u_xx = get_hessian(u_x,x)\n",
    "\n",
    "#    u_xx = get_hessian_(model,t,x)\n",
    "    \n",
    "    residual = u_t + 0.5 * torch.einsum('bii->b', sigma @ sigma.transpose(1,2) @ u_xx) + (u_x.unsqueeze(1) @ (H @ x.unsqueeze(1).transpose(1,2)) + u_x.unsqueeze(1) @ M @ alpha.unsqueeze(1).transpose(1,2) + x.unsqueeze(1) @ C @ x.unsqueeze(1).transpose(1,2) + alpha.unsqueeze(1) @ D @ alpha.unsqueeze(1).transpose(1,2)).squeeze()\n",
    "    \n",
    "    return residual\n",
    "\n",
    "def boundary_condition(model, t, x, R, T):\n",
    "\n",
    "    \n",
    "    T_input = T * torch.ones_like(t)\n",
    "\n",
    "    input = torch.cat((T_input.unsqueeze(1), x),dim=1)\n",
    "    u = model(input)\n",
    "\n",
    "    return u - (x.unsqueeze(1) @ R @ x.unsqueeze(1).transpose(1,2)).squeeze()\n",
    "\n",
    "def total_residual(model, t, x, H, M, sigma, alpha, C, D, R, T):\n",
    "    \n",
    "    residual_loss = pde_residual(model, t, x, H, M, sigma, alpha, C, D).pow(2).mean()\n",
    "    boundary_loss = boundary_condition(model, t, x, R, T).pow(2).mean()\n",
    "    \n",
    "    return residual_loss + boundary_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_data(T, num_samples):\n",
    "    # num_samples = 10000\n",
    "    t_samples = T * torch.rand(num_samples, dtype=Proj_dtype, device = Proj_device, requires_grad=False)\n",
    "    x_ends = torch.tensor([-3,3], dtype = Proj_dtype)\n",
    "    x_samples = x_ends[0] + (x_ends[1]- x_ends[0]) * torch.rand(num_samples , 2, dtype=Proj_dtype, device = Proj_device, requires_grad=False)\n",
    "    return t_samples,x_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define matrices for LQR problem\n",
    "\n",
    "H = torch.tensor([[1.2, 0.8], [-0.6, 0.9]], dtype=Proj_dtype, device = Proj_device)\n",
    "M = torch.tensor([[0.5,0.7], [0.3,1.0]], dtype=Proj_dtype, device = Proj_device)\n",
    "sigma = torch.tensor([[[0.08],[0.11]]], dtype=Proj_dtype, device = Proj_device)\n",
    "\n",
    "C = torch.tensor([[1.6, 0.0], [0.0, 1.1]], dtype=Proj_dtype, device = Proj_device)\n",
    "D = torch.tensor([[0.5, 0.0], [0.0, 0.7]], dtype=Proj_dtype, device = Proj_device)\n",
    "R = torch.tensor([[0.9, 0.0], [0.0, 1.0]], dtype=Proj_dtype, device = Proj_device)\n",
    "T = torch.tensor(1.0, dtype=Proj_dtype, device = Proj_device)\n",
    "\n",
    "solver = LQRSolver(H, M, sigma, C, D, R, T=T, method=\"euler\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_t,x_t = new_data(T,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2856, -2.5480],\n",
       "        [-1.5361, -2.1179],\n",
       "        [ 2.9114, -2.4562],\n",
       "        [-0.8773, -0.3769],\n",
       "        [ 0.0379, -0.8511]], dtype=torch.float64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_DGM_test = ControlNN_YYBver().double()\n",
    "input = torch.cat((t_t.unsqueeze(1), x_t),dim=1)\n",
    "\n",
    "a_t = control_DGM_test(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0640,  0.7670],\n",
       "        [ 0.3452,  0.8818],\n",
       "        [ 0.2939, -0.9116],\n",
       "        [-0.8331, -0.0436],\n",
       "        [ 0.0493, -0.0433]], dtype=torch.float64, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = alpha.transpose(1,2).squeeze(0)\n",
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4138]],\n",
       "\n",
       "        [[0.6039]],\n",
       "\n",
       "        [[0.6250]],\n",
       "\n",
       "        [[0.3484]],\n",
       "\n",
       "        [[0.0025]]], dtype=torch.float64, grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_t.unsqueeze(1) @ D @ a_t.unsqueeze(1).transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.2000]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha.unsqueeze(1) @ D @ alpha.unsqueeze(1).transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 9.7858]],\n",
       "\n",
       "        [[ 8.7090]],\n",
       "\n",
       "        [[20.1982]],\n",
       "\n",
       "        [[ 1.3879]],\n",
       "\n",
       "        [[ 0.7991]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_t.unsqueeze(1) @ C @ x_t.unsqueeze(1).transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[10.9858]],\n",
       "\n",
       "        [[ 9.9090]],\n",
       "\n",
       "        [[21.3982]],\n",
       "\n",
       "        [[ 2.5879]],\n",
       "\n",
       "        [[ 1.9991]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_t.unsqueeze(1) @ C @ x_t.unsqueeze(1).transpose(1,2) + alpha.transpose(1,2) @ D @ alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.2000]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha.transpose(1,2) @ D @ alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update of value function\n",
    "\n",
    "def value_update(H, M, sigma, alpha_0, model_control, C, D, R, T, iter_p): \n",
    "    if iter_p = 0\n",
    "    model_DGM = DGMNN_YYBver().double()\n",
    "    optimizer_DGM = torch.optim.Adam(model_DGM.parameters(), lr=0.0001)\n",
    "    scheduler_DGM = lr_scheduler.ExponentialLR(optimizer_DGM, gamma=0.9)\n",
    "\n",
    "\n",
    "    continue_training = input(\"Do you want to continue training or start a new one? (c/n): \").lower() == 'c'\n",
    "\n",
    "    model_save_path = 'Exercise4/model_DGM_state_dict.pt'\n",
    "    optimizer_save_path = 'Exercise4/optimizer_DGM_state.pt'\n",
    "\n",
    "    if continue_training and os.path.exists(model_save_path) and os.path.exists(optimizer_save_path):\n",
    "        \n",
    "        model_DGM.load_state_dict(torch.load(model_save_path))\n",
    "        optimizer_DGM.load_state_dict(torch.load(optimizer_save_path))\n",
    "        print(\"Continuing training from saved state.\")\n",
    "    else:\n",
    "        print(\"Starting training from scratch.\")\n",
    "\n",
    "    epoch_losses = []\n",
    "\n",
    "    iterations = 50\n",
    "    epochs = 100\n",
    "\n",
    "    patience = 10\n",
    "\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "    filename = f'Exercise4/value_training_loss_{timestamp}.dat'\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "\n",
    "        for iteration in range(iterations):\n",
    "            print(f'Iteration {iteration+1}/{iterations}'+'\\n')\n",
    "            \n",
    "            t_data,x_data = new_data(T,1000)\n",
    "            dataset = TensorDataset(t_data,x_data)\n",
    "            dataloader = DataLoader(dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "            best_loss = float('inf')\n",
    "            patience_counter = 0  \n",
    "\n",
    "            for epoch in range(epochs):\n",
    "\n",
    "                model_DGM.train()\n",
    "                total_loss = 0\n",
    "                \n",
    "                for batch_idx, (_t_data,_x_data) in enumerate(dataloader):\n",
    "                    optimizer_DGM.zero_grad()\n",
    "                    t_data = _t_data.clone().requires_grad_(True)\n",
    "                    x_data = _x_data.clone().requires_grad_(True)\n",
    "                    loss = total_residual(model_DGM, t_data, x_data, H, M, sigma, alpha_i, C, D, R, T) \n",
    "                    loss.backward()\n",
    "                    optimizer_DGM.step()\n",
    "                    total_loss += loss.item()\n",
    "                \n",
    "                avg_loss = total_loss / len(dataloader)\n",
    "                epoch_losses.append(avg_loss)\n",
    "                \n",
    "                scheduler_DGM.step()\n",
    "\n",
    "                f.write(f'Iteration {iteration+1}, Epoch {epoch+1}, Loss: {avg_loss}\\n')\n",
    "\n",
    "                if avg_loss < best_loss:\n",
    "                    best_loss = avg_loss\n",
    "                    patience_counter = 0  \n",
    "                    torch.save(model_DGM.state_dict(), model_save_path)\n",
    "                    torch.save(optimizer_DGM.state_dict(), optimizer_save_path)\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "\n",
    "                if epoch == 0 or (epoch+1) % 5 == 0:\n",
    "                    print(f'Epoch {epoch+1}/{epochs} \\t Loss: {avg_loss}')\n",
    "                \n",
    "                \n",
    "                if patience_counter >= patience:\n",
    "                    print(f'Early stopping triggered at epoch {epoch+1}')\n",
    "                    break  \n",
    "            print('\\n')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hamiltonian\n",
    "\n",
    "def Hamiltonian(model_control, model_value, t, x, H, M, sigma, C, D):\n",
    "    \n",
    "    input = torch.cat((t.unsqueeze(1), x),dim=1)\n",
    "\n",
    "    u = model_value(input)\n",
    "    a = model_control(input)\n",
    "    u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    H_i  = u_x.unsqueeze(1) @ H @ x.unsqueeze(1).transpose(1,2) + u_x.unsqueeze(1) @ M @ a.unsqueeze(1).transpose(1,2) + x.unsqueeze(1) @ C @ x.unsqueeze(1).transpose(1,2) + a.unsqueeze(1) @ D @ a.unsqueeze(1).transpose(1,2)\n",
    "\n",
    "    H  = H_i.pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update of control\n",
    "\n",
    "def control_update(H, M, sigma, alpha_i, C, D, R, T, model_value): \n",
    "\n",
    "    control_DGM = ControlNN_YYBver().double()\n",
    "    optimizer_control = torch.optim.Adam(control_DGM.parameters(), lr=0.0001)\n",
    "    scheduler_control = lr_scheduler.ExponentialLR(optimizer_control, gamma=0.9)\n",
    "\n",
    "\n",
    "    continue_training = input(\"Do you want to continue training or start a new one? (c/n): \").lower() == 'c'\n",
    "\n",
    "    model_save_path = 'Exercise4/control_DGM_state_dict.pt'\n",
    "    optimizer_save_path = 'Exercise4/optimizer_control_state.pt'\n",
    "\n",
    "    if continue_training and os.path.exists(model_save_path) and os.path.exists(optimizer_save_path):\n",
    "        \n",
    "        control_DGM.load_state_dict(torch.load(model_save_path))\n",
    "        optimizer_control.load_state_dict(torch.load(optimizer_save_path))\n",
    "        print(\"Continuing training from saved state.\")\n",
    "    else:\n",
    "        print(\"Starting training from scratch.\")\n",
    "\n",
    "    epoch_losses = []\n",
    "\n",
    "    iterations = 50\n",
    "    epochs = 100\n",
    "\n",
    "    patience = 10\n",
    "\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "    filename = f'Exercise4/control_training_loss_{timestamp}.dat'\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "\n",
    "        for iteration in range(iterations):\n",
    "            print(f'Iteration {iteration+1}/{iterations}'+'\\n')\n",
    "            \n",
    "            t_data,x_data = new_data(T,1000)\n",
    "            dataset = TensorDataset(t_data,x_data)\n",
    "            dataloader = DataLoader(dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "            best_loss = float('inf')\n",
    "            patience_counter = 0  \n",
    "\n",
    "            for epoch in range(epochs):\n",
    "\n",
    "                control_DGM.train()\n",
    "                total_loss = 0\n",
    "                \n",
    "                for batch_idx, (_t_data,_x_data) in enumerate(dataloader):\n",
    "                    optimizer_control.zero_grad()\n",
    "                    t_data = _t_data.clone().requires_grad_(True)\n",
    "                    x_data = _x_data.clone().requires_grad_(True)\n",
    "                    loss = Hamiltonian(control_DGM, model_value, t_data, x_data, H, M, sigma, C, D) \n",
    "                    loss.backward()\n",
    "                    optimizer_control.step()\n",
    "                    total_loss += loss.item()\n",
    "                \n",
    "                avg_loss = total_loss / len(dataloader)\n",
    "                epoch_losses.append(avg_loss)\n",
    "                \n",
    "                scheduler_control.step()\n",
    "\n",
    "                f.write(f'Iteration {iteration+1}, Epoch {epoch+1}, Loss: {avg_loss}\\n')\n",
    "\n",
    "                if avg_loss < best_loss:\n",
    "                    best_loss = avg_loss\n",
    "                    patience_counter = 0  \n",
    "                    torch.save(control_DGM.state_dict(), model_save_path)\n",
    "                    torch.save(optimizer_control.state_dict(), optimizer_save_path)\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "\n",
    "                if epoch == 0 or (epoch+1) % 5 == 0:\n",
    "                    print(f'Epoch {epoch+1}/{epochs} \\t Loss: {avg_loss}')\n",
    "                \n",
    "                \n",
    "                if patience_counter >= patience:\n",
    "                    print(f'Early stopping triggered at epoch {epoch+1}')\n",
    "                    break  \n",
    "            print('\\n')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Iteration\n",
    "\n",
    "iteration_p = 0\n",
    "alpha_0 = torch.tensor([[1., 1.]], dtype=Proj_dtype, device = Proj_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
