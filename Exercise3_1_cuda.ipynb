{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from lib.Exercise1_1 import LQRSolver\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import time\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "class DGMNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DGMNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(3, 100)  \n",
    "        self.layer2 = nn.Linear(100, 100)\n",
    "        self.layer3 = nn.Linear(100, 100)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(100, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.tanh(self.layer2(x))\n",
    "        x = self.relu(self.layer3(x))\n",
    "        return self.output(x)\n",
    "\n",
    "model = DGMNN().double().to(device)\n",
    "model_DGM = nn.DataParallel(model) # 使用DataParallel包裹模\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hessian(grad, x):\n",
    "\n",
    "    Hessian = torch.tensor([], device=device)  # 在创建时指定设备\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        hessian = torch.tensor([], device=device)  # 在创建时指定设备\n",
    "        for j in range(len(grad[i])):\n",
    "            u_xxi = torch.autograd.grad(grad[i][j], x, grad_outputs=torch.ones_like(grad[i][j]), retain_graph=True, create_graph=True, allow_unused=True)[0]\n",
    "            hessian = torch.cat((hessian, u_xxi[i].unsqueeze(0)))\n",
    "        Hessian = torch.cat((Hessian, hessian.unsqueeze(0)), dim=0)\n",
    "    return Hessian\n",
    "\n",
    "\n",
    "def pde_residual(model, t, x):\n",
    "    \n",
    "    input = torch.cat((t.unsqueeze(1), x),dim=1)\n",
    "    \n",
    "    u = model(input)\n",
    "\n",
    "    u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), create_graph=True, retain_graph=True)[0]\n",
    "    u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    u_xx = get_hessian(u_x,x)\n",
    "\n",
    "    residual = u_t + 0.5 * torch.einsum('bii->b', sigma @ sigma.transpose(1,2) @ u_xx) + (u_x.unsqueeze(1) @ (H @ x.unsqueeze(1).transpose(1,2)) + u_x.unsqueeze(1) @ M @ alpha + x.unsqueeze(1) @ C @ x.unsqueeze(1).transpose(1,2) + alpha.transpose(1,2) @ D @ alpha).squeeze()\n",
    "    return residual\n",
    "\n",
    "def boundary_condition(model,t, x):\n",
    "    # 定义边界条件的残差\n",
    "    \n",
    "    T_input = T * torch.ones_like(t)\n",
    "\n",
    "    input = torch.cat((T_input.unsqueeze(1), x),dim=1)\n",
    "    u = model(input)\n",
    "\n",
    "    return u - (x.unsqueeze(1) @ R @ x.unsqueeze(1).transpose(1,2)).squeeze()\n",
    "\n",
    "def total_residual(model, t, x):\n",
    "    # 计算内部PDE残差\n",
    "    residual_loss = pde_residual(model, t, x).pow(2).mean()\n",
    "    # 计算边界条件的残差\n",
    "    boundary_loss = boundary_condition(model,t,x).pow(2).mean()\n",
    "    \n",
    "    return residual_loss + boundary_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_data(num_samples):\n",
    "    #num_samples = 10000\n",
    "    t_samples = T * torch.rand(num_samples, dtype=torch.double, requires_grad=True, device = device)\n",
    "    x_ends = torch.tensor([-3,3], dtype = torch.double, device = device)\n",
    "    x_samples = x_ends[0] + (x_ends[1]- x_ends[0]) * torch.rand(num_samples , 2, dtype=torch.double, requires_grad=True, device = device)\n",
    "    return t_samples,x_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define matrices for LQR problem\n",
    "H = torch.tensor([[1.2, 0.8], [-0.6, 0.9]], dtype=torch.double).to(device)\n",
    "M = torch.tensor([[0.5,0.7], [0.3,1.0]], dtype=torch.double).to(device)\n",
    "sigma = torch.tensor([[[0.8],[1.1]]], dtype=torch.double).to(device)\n",
    "alpha = torch.tensor([[[1],[1]]], dtype=torch.double).to(device)\n",
    "C = torch.tensor([[1.6, 0.0], [0.0, 1.1]], dtype=torch.double).to(device)\n",
    "D = torch.tensor([[0.5, 0.0], [0.0, 0.7]], dtype=torch.double).to(device)\n",
    "R = torch.tensor([[0.9, 0.0], [0.0, 1.0]], dtype=torch.double).to(device)\n",
    "T = torch.tensor(1.0, dtype=torch.double).to(device)\n",
    "\n",
    "solver = LQRSolver(H, M, sigma, C, D, R, T=T, method=\"euler\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/5\n",
      "\n",
      "Epoch 1/500 \t Loss: 157.15895540540768\n",
      "Epoch 5/500 \t Loss: 156.5744525135876\n",
      "Epoch 10/500 \t Loss: 157.2343503824876\n",
      "Epoch 15/500 \t Loss: 157.0352401497688\n",
      "Epoch 20/500 \t Loss: 157.5177927261907\n",
      "Epoch 25/500 \t Loss: 158.15772361604962\n",
      "Epoch 30/500 \t Loss: 157.456010234717\n",
      "Epoch 35/500 \t Loss: 158.66914723811715\n",
      "Epoch 40/500 \t Loss: 158.00951783855714\n",
      "Residual is below threshold, starting a new training round.\n",
      "\n",
      "\n",
      "Batch 2/5\n",
      "\n",
      "Epoch 1/500 \t Loss: 159.32740969957877\n",
      "Epoch 5/500 \t Loss: 158.7518501420791\n",
      "Epoch 10/500 \t Loss: 158.6615372347826\n",
      "Epoch 15/500 \t Loss: 158.74475132422032\n",
      "Epoch 20/500 \t Loss: 158.27082690812614\n",
      "Epoch 25/500 \t Loss: 158.8103050811938\n",
      "Epoch 30/500 \t Loss: 158.96456274925154\n",
      "Epoch 35/500 \t Loss: 158.01106209673006\n",
      "Epoch 40/500 \t Loss: 159.66086135376582\n",
      "Epoch 45/500 \t Loss: 159.62037069328113\n",
      "Epoch 50/500 \t Loss: 159.38795703444987\n",
      "Epoch 55/500 \t Loss: 158.61744021382984\n",
      "Epoch 60/500 \t Loss: 157.9369798305605\n",
      "Epoch 65/500 \t Loss: 159.16168516269704\n",
      "Epoch 70/500 \t Loss: 159.63599645047069\n",
      "Epoch 75/500 \t Loss: 158.3894792455691\n",
      "Epoch 80/500 \t Loss: 159.0145373974737\n",
      "Epoch 85/500 \t Loss: 159.05598197008962\n",
      "Epoch 90/500 \t Loss: 159.18099468843775\n",
      "Epoch 95/500 \t Loss: 159.53740920406818\n",
      "Epoch 100/500 \t Loss: 159.34981572391735\n",
      "Epoch 105/500 \t Loss: 159.02974283134407\n",
      "Epoch 110/500 \t Loss: 158.61211719830055\n",
      "Epoch 115/500 \t Loss: 158.79901776046958\n",
      "Epoch 120/500 \t Loss: 160.3804138565327\n",
      "Epoch 125/500 \t Loss: 159.04207176770308\n",
      "Epoch 130/500 \t Loss: 159.6287233703053\n",
      "Epoch 135/500 \t Loss: 158.2215774322276\n",
      "Epoch 140/500 \t Loss: 159.22909586969052\n",
      "Epoch 145/500 \t Loss: 158.92038795416153\n",
      "Epoch 150/500 \t Loss: 159.81573080437906\n",
      "Epoch 155/500 \t Loss: 157.76005190700943\n",
      "Epoch 160/500 \t Loss: 159.4467014750913\n",
      "Epoch 165/500 \t Loss: 158.819988298805\n",
      "Epoch 170/500 \t Loss: 159.31735874061178\n",
      "Epoch 175/500 \t Loss: 159.91290930996664\n",
      "Epoch 180/500 \t Loss: 159.6633925732957\n",
      "Epoch 185/500 \t Loss: 158.99877921407213\n",
      "Epoch 190/500 \t Loss: 159.11110215199585\n",
      "Epoch 195/500 \t Loss: 158.74622139736528\n",
      "Epoch 200/500 \t Loss: 158.80685539884027\n",
      "Epoch 205/500 \t Loss: 159.24912850090152\n",
      "Epoch 210/500 \t Loss: 158.89699346201795\n",
      "Epoch 215/500 \t Loss: 158.99909616177908\n",
      "Epoch 220/500 \t Loss: 158.92755122632968\n",
      "Epoch 225/500 \t Loss: 158.4044323433021\n",
      "Epoch 230/500 \t Loss: 158.6687638967414\n",
      "Epoch 235/500 \t Loss: 158.8597183759919\n",
      "Epoch 240/500 \t Loss: 159.40957914561616\n",
      "Epoch 245/500 \t Loss: 158.83900115864324\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m t_data_, x_data_ \u001b[38;5;241m=\u001b[39m t_data_\u001b[38;5;241m.\u001b[39mto(device), x_data_\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# 数据转移到GPU\u001b[39;00m\n\u001b[0;32m     28\u001b[0m optimizer_DGM\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 29\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtotal_residual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_DGM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_data_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_data_\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     30\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward\n\u001b[0;32m     31\u001b[0m optimizer_DGM\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[2], line 40\u001b[0m, in \u001b[0;36mtotal_residual\u001b[1;34m(model, t, x)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtotal_residual\u001b[39m(model, t, x):\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# 计算内部PDE残差\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     residual_loss \u001b[38;5;241m=\u001b[39m \u001b[43mpde_residual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# 计算边界条件的残差\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     boundary_loss \u001b[38;5;241m=\u001b[39m boundary_condition(model,t,x)\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n",
      "Cell \u001b[1;32mIn[2], line 23\u001b[0m, in \u001b[0;36mpde_residual\u001b[1;34m(model, t, x)\u001b[0m\n\u001b[0;32m     20\u001b[0m u_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(u, t, grad_outputs\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones_like(u), create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     21\u001b[0m u_x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(u, x, grad_outputs\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones_like(u), create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 23\u001b[0m u_xx \u001b[38;5;241m=\u001b[39m \u001b[43mget_hessian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m residual \u001b[38;5;241m=\u001b[39m u_t \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbii->b\u001b[39m\u001b[38;5;124m'\u001b[39m, sigma \u001b[38;5;241m@\u001b[39m sigma\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m@\u001b[39m u_xx) \u001b[38;5;241m+\u001b[39m (u_x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m@\u001b[39m (H \u001b[38;5;241m@\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)) \u001b[38;5;241m+\u001b[39m u_x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m@\u001b[39m M \u001b[38;5;241m@\u001b[39m alpha \u001b[38;5;241m+\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m@\u001b[39m C \u001b[38;5;241m@\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m alpha\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m@\u001b[39m D \u001b[38;5;241m@\u001b[39m alpha)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m, in \u001b[0;36mget_hessian\u001b[1;34m(grad, x)\u001b[0m\n\u001b[0;32m      6\u001b[0m hessian \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([], device\u001b[38;5;241m=\u001b[39mdevice)  \u001b[38;5;66;03m# 在创建时指定设备\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(grad[i])):\n\u001b[1;32m----> 8\u001b[0m     u_xxi \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      9\u001b[0m     hessian \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((hessian, u_xxi[i]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)))\n\u001b[0;32m     10\u001b[0m Hessian \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((Hessian, hessian\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\warre\\Documents\\SCDAA-1\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:411\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[0;32m    407\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[0;32m    408\u001b[0m         grad_outputs_\n\u001b[0;32m    409\u001b[0m     )\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 411\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m    422\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[0;32m    423\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[0;32m    424\u001b[0m     ):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 准备训练\n",
    "optimizer_DGM = torch.optim.Adam(model_DGM.parameters(), lr=0.001)\n",
    "epoch_losses = []\n",
    "\n",
    "Batch_size = 5\n",
    "epochs = 500\n",
    "\n",
    "# 设置残差下降的阈值\n",
    "residual_threshold = 0.005\n",
    "min_residual = float('inf')\n",
    "for batch in range(Batch_size):\n",
    "    print(f'Batch {batch+1}/{Batch_size}\\n')\n",
    "    \n",
    "    # 假设new_data函数返回的是适合CUDA的数据，或者需要在这里将数据转移到GPU\n",
    "    t_data, x_data = new_data(3000)\n",
    "    t_data, x_data = t_data.to(device), x_data.to(device)  # 数据转移到GPU\n",
    "    \n",
    "    dataset = TensorDataset(t_data, x_data)\n",
    "    dataloader = DataLoader(dataset, batch_size=350, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model_DGM.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_idx, (t_data_, x_data_) in enumerate(dataloader):\n",
    "            t_data_, x_data_ = t_data_.to(device), x_data_.to(device)  # 数据转移到GPU\n",
    "            \n",
    "            optimizer_DGM.zero_grad()\n",
    "            loss = total_residual(model_DGM, t_data_, x_data_) \n",
    "            loss.backward\n",
    "            optimizer_DGM.step()\n",
    "            total_loss += loss.item()\n",
    "        epoch_losses.append(total_loss / len(dataloader))\n",
    "        if len(epoch_losses) < 2:\n",
    "            average_residual = abs(epoch_losses[-1] - 0)\n",
    "        else:\n",
    "            average_residual = abs(epoch_losses[-1] - epoch_losses[-2]) \n",
    "        if epoch == 0:\n",
    "            print(f'Epoch {epoch+1}/{epochs} \\t Loss: {total_loss / len(dataloader)}')\n",
    "        \n",
    "        if (epoch+1) % 5 == 0:\n",
    "            print(f'Epoch {epoch+1}/{epochs} \\t Loss: {total_loss / len(dataloader)}')\n",
    "    # 检查残差是否下降到阈值以下\n",
    "        if average_residual < residual_threshold:\n",
    "            print(\"Residual is below threshold, starting a new training round.\")\n",
    "            break  # 结束当前训练轮次\n",
    "        \n",
    "        # 可选：更新min_residual\n",
    "        if average_residual < min_residual:\n",
    "            min_residual = average_residual\n",
    "    print('\\n')\n",
    "\n",
    "model_DGM.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 179.38920943757603\n",
      "Epoch 100/500 \t Loss: 55.63965676485591\n",
      "Epoch 200/500 \t Loss: 21.000615344534598\n",
      "Epoch 300/500 \t Loss: 16.933412291149832\n",
      "Epoch 400/500 \t Loss: 15.0249363441466\n",
      "Epoch 500/500 \t Loss: 13.844563649177934\n",
      "Batch 2/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 19.503522725018385\n",
      "Epoch 100/500 \t Loss: 15.59205568977103\n",
      "Epoch 200/500 \t Loss: 14.961941164535304\n",
      "Epoch 300/500 \t Loss: 14.578325733578623\n",
      "Epoch 400/500 \t Loss: 14.280125460164486\n",
      "Epoch 500/500 \t Loss: 14.020513356986141\n",
      "Batch 3/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 25.340556877811157\n",
      "Epoch 100/500 \t Loss: 19.11800524720242\n",
      "Epoch 200/500 \t Loss: 18.613381802375137\n",
      "Epoch 300/500 \t Loss: 18.314198112088807\n",
      "Epoch 400/500 \t Loss: 18.106350705380343\n",
      "Epoch 500/500 \t Loss: 17.96623103348157\n",
      "Batch 4/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 18.316024904658608\n",
      "Epoch 100/500 \t Loss: 14.321167523864505\n",
      "Epoch 200/500 \t Loss: 14.088331209657527\n",
      "Epoch 300/500 \t Loss: 13.95257659499725\n",
      "Epoch 400/500 \t Loss: 13.880596541372851\n",
      "Epoch 500/500 \t Loss: 13.838540642104649\n",
      "Batch 5/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 16.603201980423833\n",
      "Epoch 100/500 \t Loss: 10.893308765419468\n",
      "Epoch 200/500 \t Loss: 10.799006848406007\n",
      "Epoch 300/500 \t Loss: 10.76169036124822\n",
      "Epoch 400/500 \t Loss: 10.740574049237525\n",
      "Epoch 500/500 \t Loss: 10.725385042820179\n",
      "Batch 6/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 21.0254261161987\n",
      "Epoch 100/500 \t Loss: 12.77113418758258\n",
      "Epoch 200/500 \t Loss: 12.691954921918315\n",
      "Epoch 300/500 \t Loss: 12.654403327316457\n",
      "Epoch 400/500 \t Loss: 12.633358100252506\n",
      "Epoch 500/500 \t Loss: 12.620517919690661\n",
      "Batch 7/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 19.22261260790122\n",
      "Epoch 100/500 \t Loss: 11.854642696797141\n",
      "Epoch 200/500 \t Loss: 11.806748574359398\n",
      "Epoch 300/500 \t Loss: 11.787555029846107\n",
      "Epoch 400/500 \t Loss: 11.776965983310944\n",
      "Epoch 500/500 \t Loss: 11.770306922797884\n",
      "Batch 8/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 17.70131773985912\n",
      "Epoch 100/500 \t Loss: 14.267154729980172\n",
      "Epoch 200/500 \t Loss: 14.204802726323136\n",
      "Epoch 300/500 \t Loss: 14.189355026344703\n",
      "Epoch 400/500 \t Loss: 14.18399222345668\n",
      "Epoch 500/500 \t Loss: 14.18108874727821\n",
      "Batch 9/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 14.405415098152291\n",
      "Epoch 100/500 \t Loss: 12.15503977986431\n",
      "Epoch 200/500 \t Loss: 12.13593010982086\n",
      "Epoch 300/500 \t Loss: 12.13070571261003\n",
      "Epoch 400/500 \t Loss: 12.12851968289012\n",
      "Epoch 500/500 \t Loss: 12.127126819438834\n",
      "Batch 10/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 17.076769226688942\n",
      "Epoch 100/500 \t Loss: 12.650131537774369\n",
      "Epoch 200/500 \t Loss: 12.621707882321694\n",
      "Epoch 300/500 \t Loss: 12.61170053249242\n",
      "Epoch 400/500 \t Loss: 12.607536671351454\n",
      "Epoch 500/500 \t Loss: 12.605548523226597\n",
      "Batch 11/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 17.879089137743428\n",
      "Epoch 100/500 \t Loss: 11.388772659728536\n",
      "Epoch 200/500 \t Loss: 11.346630834342934\n",
      "Epoch 300/500 \t Loss: 11.332133713612906\n",
      "Epoch 400/500 \t Loss: 11.325073311817595\n",
      "Epoch 500/500 \t Loss: 11.320870767780143\n",
      "Batch 12/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 18.192793433834126\n",
      "Epoch 100/500 \t Loss: 15.824185397319265\n",
      "Epoch 200/500 \t Loss: 15.787821839306956\n",
      "Epoch 300/500 \t Loss: 15.778305067565379\n",
      "Epoch 400/500 \t Loss: 15.774780168344245\n",
      "Epoch 500/500 \t Loss: 15.7729576774973\n",
      "Batch 13/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 15.12153911170846\n",
      "Epoch 100/500 \t Loss: 12.969719917332302\n",
      "Epoch 200/500 \t Loss: 12.954158350955115\n",
      "Epoch 300/500 \t Loss: 12.947761605282874\n",
      "Epoch 400/500 \t Loss: 12.944559217183782\n",
      "Epoch 500/500 \t Loss: 12.9425420557071\n",
      "Batch 14/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 17.428828288865482\n",
      "Epoch 100/500 \t Loss: 15.004155484798455\n",
      "Epoch 200/500 \t Loss: 14.993899121782515\n",
      "Epoch 300/500 \t Loss: 14.989638230589938\n",
      "Epoch 400/500 \t Loss: 14.987351396159916\n",
      "Epoch 500/500 \t Loss: 14.985935458104187\n",
      "Batch 15/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 21.67395585378349\n",
      "Epoch 100/500 \t Loss: 19.130457348665402\n",
      "Epoch 200/500 \t Loss: 19.111021064626097\n",
      "Epoch 300/500 \t Loss: 19.10463702228961\n",
      "Epoch 400/500 \t Loss: 19.101376893511357\n",
      "Epoch 500/500 \t Loss: 19.099403028586366\n",
      "Batch 16/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 15.839748087205068\n",
      "Epoch 100/500 \t Loss: 13.81575445748356\n",
      "Epoch 200/500 \t Loss: 13.804348901987217\n",
      "Epoch 300/500 \t Loss: 13.801507605187632\n",
      "Epoch 400/500 \t Loss: 13.800227820706219\n",
      "Epoch 500/500 \t Loss: 13.799421370277082\n",
      "Batch 17/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 14.330985374106621\n",
      "Epoch 100/500 \t Loss: 10.938275950109585\n",
      "Epoch 200/500 \t Loss: 10.931499582105687\n",
      "Epoch 300/500 \t Loss: 10.9289099191339\n",
      "Epoch 400/500 \t Loss: 10.927363219733197\n",
      "Epoch 500/500 \t Loss: 10.926270126546994\n",
      "Batch 18/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 17.217842260171746\n",
      "Epoch 100/500 \t Loss: 15.838128275821221\n",
      "Epoch 200/500 \t Loss: 15.823587394772606\n",
      "Epoch 300/500 \t Loss: 15.818064006232476\n",
      "Epoch 400/500 \t Loss: 15.814796954362\n",
      "Epoch 500/500 \t Loss: 15.812622448193757\n",
      "Batch 19/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 15.818213782627524\n",
      "Epoch 100/500 \t Loss: 13.42515321617837\n",
      "Epoch 200/500 \t Loss: 13.415542270759731\n",
      "Epoch 300/500 \t Loss: 13.41243184755704\n",
      "Epoch 400/500 \t Loss: 13.411105000598141\n",
      "Epoch 500/500 \t Loss: 13.410457584962652\n",
      "Batch 20/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 11.230250088660055\n",
      "Epoch 100/500 \t Loss: 9.411603714577025\n",
      "Epoch 200/500 \t Loss: 9.400992662962922\n",
      "Epoch 300/500 \t Loss: 9.397263971337486\n",
      "Epoch 400/500 \t Loss: 9.395333204864055\n",
      "Epoch 500/500 \t Loss: 9.39413285813998\n",
      "Batch 21/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 11.923259699628279\n",
      "Epoch 100/500 \t Loss: 10.079914843287808\n",
      "Epoch 200/500 \t Loss: 10.068834999968193\n",
      "Epoch 300/500 \t Loss: 10.065174731579814\n",
      "Epoch 400/500 \t Loss: 10.063609715677979\n",
      "Epoch 500/500 \t Loss: 10.062831174918266\n",
      "Batch 22/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 13.524170602346674\n",
      "Epoch 100/500 \t Loss: 12.44271573204784\n",
      "Epoch 200/500 \t Loss: 12.437439794988011\n",
      "Epoch 300/500 \t Loss: 12.434824979040881\n",
      "Epoch 400/500 \t Loss: 12.433322883921997\n",
      "Epoch 500/500 \t Loss: 12.432401185857836\n",
      "Batch 23/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 10.921914816427755\n",
      "Epoch 100/500 \t Loss: 10.152609573353306\n",
      "Epoch 200/500 \t Loss: 10.148826486471584\n",
      "Epoch 300/500 \t Loss: 10.147547726876777\n",
      "Epoch 400/500 \t Loss: 10.14696523453958\n",
      "Epoch 500/500 \t Loss: 10.146632403694523\n",
      "Batch 24/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 20.24021652350002\n",
      "Epoch 100/500 \t Loss: 18.055442250345322\n",
      "Epoch 200/500 \t Loss: 18.049309440131598\n",
      "Epoch 300/500 \t Loss: 18.04610508760644\n",
      "Epoch 400/500 \t Loss: 18.04403459533716\n",
      "Epoch 500/500 \t Loss: 18.042642271151472\n",
      "Batch 25/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 16.818568380085843\n",
      "Epoch 100/500 \t Loss: 15.029096630993733\n",
      "Epoch 200/500 \t Loss: 15.022937190112058\n",
      "Epoch 300/500 \t Loss: 15.020379733945404\n",
      "Epoch 400/500 \t Loss: 15.019018655927374\n",
      "Epoch 500/500 \t Loss: 15.018195033539751\n",
      "Batch 26/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 13.904689424309534\n",
      "Epoch 100/500 \t Loss: 10.95374325142318\n",
      "Epoch 200/500 \t Loss: 10.947914167650236\n",
      "Epoch 300/500 \t Loss: 10.946182000318519\n",
      "Epoch 400/500 \t Loss: 10.945504307201627\n",
      "Epoch 500/500 \t Loss: 10.945171571373075\n",
      "Batch 27/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 17.126471635678392\n",
      "Epoch 100/500 \t Loss: 15.520214585780147\n",
      "Epoch 200/500 \t Loss: 15.509972885680435\n",
      "Epoch 300/500 \t Loss: 15.505880205856196\n",
      "Epoch 400/500 \t Loss: 15.50395921895383\n",
      "Epoch 500/500 \t Loss: 15.502969857620636\n",
      "Batch 28/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 14.824829943732531\n",
      "Epoch 100/500 \t Loss: 13.649976110050158\n",
      "Epoch 200/500 \t Loss: 13.64611006148198\n",
      "Epoch 300/500 \t Loss: 13.645038495106963\n",
      "Epoch 400/500 \t Loss: 13.644617444516044\n",
      "Epoch 500/500 \t Loss: 13.644399030959615\n",
      "Batch 29/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 12.001834386220537\n",
      "Epoch 100/500 \t Loss: 9.98238320194046\n",
      "Epoch 200/500 \t Loss: 9.976862938181611\n",
      "Epoch 300/500 \t Loss: 9.974973102175106\n",
      "Epoch 400/500 \t Loss: 9.974069938138461\n",
      "Epoch 500/500 \t Loss: 9.973547992460126\n",
      "Batch 30/30\n",
      "\n",
      "Epoch 1/500 \t Loss: 13.946458958981626\n",
      "Epoch 100/500 \t Loss: 12.215250434477516\n",
      "Epoch 200/500 \t Loss: 12.212002043167447\n",
      "Epoch 300/500 \t Loss: 12.210672167365013\n",
      "Epoch 400/500 \t Loss: 12.209912113896316\n",
      "Epoch 500/500 \t Loss: 12.209413820040151\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DGMNN(\n",
       "  (layer1): Linear(in_features=3, out_features=50, bias=True)\n",
       "  (layer2): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (tanh): Tanh()\n",
       "  (output): Linear(in_features=50, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_DGM = DGMNN().double()\n",
    "\n",
    "# Prepare for training\n",
    "optimizer_DGM = torch.optim.Adam(model_DGM.parameters(), lr=0.001)\n",
    "epoch_losses = []\n",
    "\n",
    "batch_size = 30\n",
    "epochs = 500\n",
    "\n",
    "for batch in range(batch_size):\n",
    "    print(f'Batch {batch+1}/{batch_size}'+'\\n')\n",
    "    \n",
    "    t_data,x_data = new_data(50)\n",
    "    dataset = TensorDataset(t_data,x_data)\n",
    "    dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        model_DGM.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_idx, (t_data_,x_data_) in enumerate(dataloader):\n",
    "            optimizer_DGM.zero_grad()\n",
    "            loss = total_residual(model_DGM, t_data_, x_data_) \n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer_DGM.step()\n",
    "            total_loss += loss.item()\n",
    "        epoch_losses.append(total_loss / len(dataloader))\n",
    "\n",
    "        if epoch == 0:\n",
    "            print(f'Epoch {epoch+1}/{epochs} \\t Loss: {total_loss / len(dataloader)}')\n",
    "        if (epoch+1) % 100 == 0:\n",
    "            print(f'Epoch {epoch+1}/{epochs} \\t Loss: {total_loss / len(dataloader)}')\n",
    "\n",
    "print('\\n')\n",
    "model_DGM.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "# Define matrices for LQR problem\n",
    "H = torch.tensor([[1.2, 0.8], [-0.6, 0.9]], dtype=torch.double).to(device)\n",
    "M = torch.tensor([[0.5,0.7], [0.3,1.0]], dtype=torch.double).to(device)\n",
    "sigma = torch.tensor([[[0.8],[1.1]]], dtype=torch.double).to(device)\n",
    "alpha = torch.tensor([[[1],[1]]], dtype=torch.double).to(device)\n",
    "C = torch.tensor([[1.6, 0.0], [0.0, 1.1]], dtype=torch.double).to(device)\n",
    "D = torch.tensor([[0.5, 0.0], [0.0, 0.7]], dtype=torch.double).to(device)\n",
    "R = torch.tensor([[0.9, 0.0], [0.0, 1.0]], dtype=torch.double).to(device)\n",
    "T = torch.tensor(1.0, dtype=torch.double).to(device)\n",
    "\n",
    "solver = LQRSolver(H, M, sigma, C, D, R, T=T, method=\"euler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "t_data,x_data = new_data(5)\n",
    "\n",
    "value_numerical = solver.value_function(t_data.detach(),x_data.detach().unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = model_DGM(torch.cat((t_data.unsqueeze(1), x_data.squeeze(1)),dim=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.7449,  8.7295,  8.4844, 10.8576,  6.6628], device='cuda:0',\n",
       "       dtype=torch.float64, grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.6655,  1.5675,  7.7832,  9.9292, 11.0548], dtype=torch.float64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_numerical"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
